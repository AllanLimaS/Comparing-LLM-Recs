{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuração inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from Utils import templates, utils, recommender, models_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de Usuários: 943\n"
     ]
    }
   ],
   "source": [
    "## Configurações base\n",
    "\n",
    "config = {\n",
    "    #\"LLM_runtime\": \"ROCm llama.cpp v1.28.0\", \n",
    "    #\"LLM_runtime\": \"CPU llama.cpp v1.22.2\", # performance ruim\n",
    "    \"LLM_runtime\": \"Vulkan llama.cpp v1.28.0\", # melhor opção\n",
    "    \"dataset\": \"ml_100k\",                                                  #| Opções 'ml_100k' e 'ml_1m'\n",
    "    \"nsu\" : 12,     # número de usuários para filtragem colaborativa        | SSBD :12  | Default :18   |            | Best = 19\n",
    "    \"nci\" :19,      # número de itens para filtragem colaborativa           | SSBD :19  | Default :24   | Max : 1682 |\n",
    "    \"lenlimit\" : 8,  # limite de tamanho para a lista filmes assistidos     | SSBD : 8  | Default :24   | Max : 1682 | Best = 8\n",
    "    \"lenlimit_option\" : 'primeiros', # define qual a abordagem                | opções : 'ultimos', 'primeiros', 'aleatorio' | Default : 'primeiros'\n",
    "    \"test_run\" : 0, # define a quantidade de recomendações,                 |           | Default :0    | Max : 943 \n",
    "    \"obs\": \"base\"\n",
    "}\n",
    "\n",
    "## define o prompt template\n",
    "prompt_template = templates.PROMPT_TEMPLATE_3\n",
    "config.update({\"prompt_template\": prompt_template})\n",
    "\n",
    "## define o prompt para formatar a resposta final \n",
    "#prompt_format = templates.PROMPT_TEMPLATE_ESTRUCTURE\n",
    "prompt_format = \"\" # para não utilizar \n",
    "config.update({\"prompt_format\": prompt_format})\n",
    "\n",
    "# load movie lens 100k dataset\n",
    "dataset = utils.read_json(f\"Data/{config['dataset']}.json\")\n",
    "print(f'Quantidade de Usuários: {len(dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config.update(models_config.qwen_2_5_7b_instruct_1m())\n",
    "config.update(models_config.qwen_2_5_7b_instruct_1m())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execução unitária"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "514c7763ef104b76a269c2a04b292c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processando:   0%|          | 0/943 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_pkl = recommender.recommendation_workflow_new(config         = config,\n",
    "                                                 dataset        = dataset,\n",
    "                                                 prompt_template= prompt_template,\n",
    "                                                 prompt_format  = prompt_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58f4cffac2aa4a288c425995233909a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processando:   0%|          | 0/943 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_pkl = recommender.recommendation_workflow_new(config         = config,\n",
    "                                                 dataset        = dataset,\n",
    "                                                 prompt_template= prompt_template,\n",
    "                                                 prompt_format  = prompt_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execução multipla "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(config_list)):\n",
    "    config = config_list[i]\n",
    "    print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list = []\n",
    "\n",
    "## configuração \"primeiros\"\n",
    "\n",
    "config1 = config.copy()\n",
    "config1.update(models_config.qwen_2_5_7b_instruct_1m())\n",
    "config1.update({\"lenlimit_option\" : \"primeiros\"})\n",
    "config_list.append(config1)\n",
    "\n",
    "config2 = config.copy()\n",
    "config2.update(models_config.gemma_3_4b_it())\n",
    "config2.update({\"lenlimit_option\" : \"primeiros\"})\n",
    "config_list.append(config2)\n",
    "\n",
    "## configuração \"Ultimos\"\n",
    "\n",
    "config3 = config.copy()\n",
    "config3.update(models_config.qwen_2_5_7b_instruct_1m())\n",
    "config3.update({\"lenlimit_option\" : \"ultimos\"})\n",
    "\n",
    "config_list.append(config3)\n",
    "\n",
    "config4 = config.copy()\n",
    "config4.update(models_config.gemma_3_4b_it())\n",
    "config4.update({\"lenlimit_option\" : \"ultimos\"})\n",
    "config_list.append(config4)\n",
    "\n",
    "## configuração \"Aleatorios\"\n",
    "\n",
    "config5 = config.copy()\n",
    "config5.update(models_config.qwen_2_5_7b_instruct_1m())\n",
    "config5.update({\"lenlimit_option\" : \"aleatorio\"})\n",
    "config_list.append(config5)\n",
    "\n",
    "config6 = config.copy()\n",
    "config6.update(models_config.gemma_3_4b_it())\n",
    "config6.update({\"lenlimit_option\" : \"aleatorio\"})\n",
    "config_list.append(config6)\n",
    "\n",
    "## testando fine tunning\n",
    "\n",
    "config7 = config.copy()\n",
    "config7.update(models_config.gemma_3_4b_it())\n",
    "config7.update({\"lenlimit_option\" : \"ultimos\"})\n",
    "config7.update({\"obs\" : \"28/04/2025 testando fine tunning\"})\n",
    "config_list.append(config7)\n",
    "\n",
    "config8 = config.copy()\n",
    "config8.update(models_config.gemma_3_4b_ft_allan())\n",
    "config8.update({\"lenlimit_option\" : \"ultimos\"})\n",
    "config8.update({\"obs\" : \"28/04/2025 testando fine tunning\"})\n",
    "config_list.append(config8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list = []\n",
    "\n",
    "## configuração \"primeiros\"\n",
    "\n",
    "config1 = config.copy()\n",
    "config1.update(models_config.qwen_2_5_7b_instruct_1m())\n",
    "config1.update({\"lenlimit_option\" : \"primeiros\"})\n",
    "config_list.append(config1)\n",
    "\n",
    "config2 = config.copy()\n",
    "config2.update(models_config.gemma_3_4b_it())\n",
    "config2.update({\"lenlimit_option\" : \"primeiros\"})\n",
    "config_list.append(config2)\n",
    "\n",
    "## configuração \"Ultimos\"\n",
    "\n",
    "config3 = config.copy()\n",
    "config3.update(models_config.qwen_2_5_7b_instruct_1m())\n",
    "config3.update({\"lenlimit_option\" : \"ultimos\"})\n",
    "\n",
    "config_list.append(config3)\n",
    "\n",
    "config4 = config.copy()\n",
    "config4.update(models_config.gemma_3_4b_it())\n",
    "config4.update({\"lenlimit_option\" : \"ultimos\"})\n",
    "config_list.append(config4)\n",
    "\n",
    "## configuração \"Aleatorios\"\n",
    "\n",
    "config5 = config.copy()\n",
    "config5.update(models_config.qwen_2_5_7b_instruct_1m())\n",
    "config5.update({\"lenlimit_option\" : \"aleatorio\"})\n",
    "config_list.append(config5)\n",
    "\n",
    "config6 = config.copy()\n",
    "config6.update(models_config.gemma_3_4b_it())\n",
    "config6.update({\"lenlimit_option\" : \"aleatorio\"})\n",
    "config_list.append(config6)\n",
    "\n",
    "## testando fine tunning\n",
    "\n",
    "config7 = config.copy()\n",
    "config7.update(models_config.gemma_3_4b_it())\n",
    "config7.update({\"lenlimit_option\" : \"ultimos\"})\n",
    "config7.update({\"obs\" : \"28/04/2025 testando fine tunning\"})\n",
    "config_list.append(config7)\n",
    "\n",
    "config8 = config.copy()\n",
    "config8.update(models_config.gemma_3_4b_ft_allan())\n",
    "config8.update({\"lenlimit_option\" : \"ultimos\"})\n",
    "config8.update({\"obs\" : \"28/04/2025 testando fine tunning\"})\n",
    "config_list.append(config8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(config_list)):\n",
    "    config = config_list[i]\n",
    "    dataset = utils.read_json(f\"Data/{config['dataset']}.json\")\n",
    "    print(f'Rodando configuração {i} de {len(config_list)}')\n",
    "    try:\n",
    "        result_pkl = recommender.recommendation_workflow(config         = config,\n",
    "                                                         dataset        = dataset,\n",
    "                                                        prompt_template= prompt_template,\n",
    "                                                        prompt_format  = prompt_format)\n",
    "        print(f'Configuração {i} de {len(config_list)} finalizada')\n",
    "    except Exception as e:\n",
    "        print(f'Erro na configuração {i} de {len(config_list)}')\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# outros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retorna apenas o usuários que tem o gt no candidatos.\n",
    "# implementar sobre o pipeline do projeto \n",
    "\n",
    "def get_candidate_ids_list(data, id_list, user_matrix_sim, num_u, num_i):\n",
    "    cand_ids = []\n",
    "    for i in id_list:\n",
    "        watched_movies = data[i][0].split(' | ')\n",
    "        candidate_items = utils.sort_user_filtering_items(data, watched_movies, user_matrix_sim[i], num_u, num_i)\n",
    "        if data[i][-1] in candidate_items:\n",
    "            cand_ids.append(i)\n",
    "    return cand_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = list(range(0, len(dataset)))\n",
    "#assert(len(id_list) == 943) # aqui é verificado se a lista possue exatamente essa quantidade\n",
    "\n",
    "# Building indexes and similarity matrices for users and movies.\n",
    "movie_idx = utils.build_moviename_index_dict(dataset)\n",
    "user_sim_matrix = utils.build_user_similarity_matrix(dataset, movie_idx)\n",
    "\n",
    "\n",
    "# para fazer a filtragem sobre os filmes \n",
    "#pop_dict = utils.build_movie_popularity_dict(dataset) \n",
    "#item_sim_matrix = utils.build_item_similarity_matrix(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pkl = recommender.recommendation_workflow(config         = config,\n",
    "                                                 dataset        = dataset,\n",
    "                                                 prompt_template= prompt_template,\n",
    "                                                 prompt_format  = prompt_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## execução varias configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from Utils import templates, utils, recommender\n",
    "\n",
    "## PROMPT 2 \n",
    "\n",
    "config = {}\n",
    "\n",
    "## define o prompt template\n",
    "prompt_template = templates.PROMPT_TEMPLATE_2\n",
    "config.update({\"prompt_template\": prompt_template})\n",
    "\n",
    "## define o prompt para formatar a resposta final \n",
    "#prompt_format = templates.PROMPT_TEMPLATE_ESTRUCTURE\n",
    "prompt_format = \"\" # para não utilizar \n",
    "config.update({\"prompt_format\": prompt_format})\n",
    "\n",
    "# load movie lens 100k dataset\n",
    "dataset = utils.read_json(\"Data/ML100K_clean.json\")\n",
    "print(f'Quantidade de Usuários: {len(dataset)}')\n",
    "\n",
    "config.update({\n",
    "    #\"runtime\": \"ROCm llama.cpp v1.23.0\", \n",
    "    #\"runtime\": \"CPU llama.cpp v1.22.2\", # performance ruim\n",
    "    \"LLM_runtime\": \"Vulkan llama.cpp v1.23.0\", # melhor opção\n",
    "    \"dataset\": \"ml_100k\",\n",
    "    \"nsu\" : 19,     # número de usuários para filtragem colaborativa        | SSBD :12  | Default :18   | \n",
    "    \"nci\" :19,      # número de itens para filtragem colaborativa           | SSBD :19  | Default :24   | Max : 1682\n",
    "    \"lenlimit\" : 8,  # limite de tamanho para a lista filmes assistidos    | SSBD : 8  | Default :24   | Max : 1682\n",
    "    \"test_run\" : 0, # define a quantidade de recomendações,                 |           | Default :0    | Max : 943 \n",
    "    \"obs\": \"testando prompt novo - prompt2\"\n",
    "})\n",
    "\n",
    "config.update({\n",
    "    \"model_name\" :\"gemma-3-4b-it\",\n",
    "    \"Arch\" : \"gemma3\",\n",
    "    \"Quantization\" : \"Q4_K_M\",\n",
    "    \"Temperature\": 0.1,\n",
    "    \"max_tokens\" : -1,  # Default : 4096\n",
    "    \"GPU Offload\": 34,\n",
    "    \"CPU Thread Pool Size\": 6,\n",
    "    \"Evaluation Batch Size\": 512,\n",
    "    \"Flash Attention\": False, # não vi vantagem no uso \n",
    "})\n",
    "\n",
    "config_list = []\n",
    "\n",
    "config1 = config.copy()\n",
    "config_list.append(config1)\n",
    "\n",
    "for i in range(0, len(config_list)):\n",
    "    config = config_list[i]\n",
    "    print(f'Rodando configuração {i} de {len(config_list)}')\n",
    "    try:\n",
    "        result_pkl = recommender.recommendation_workflow(config         = config,\n",
    "                                                         dataset        = dataset,\n",
    "                                                        prompt_template= prompt_template,\n",
    "                                                        prompt_format  = prompt_format)\n",
    "        print(f'Configuração {i} de {len(config_list)} finalizada')\n",
    "    except Exception as e:\n",
    "        print(f'Erro na configuração {i} de {len(config_list)}')\n",
    "        print(e)\n",
    "        continue\n",
    "\n",
    "## PROMPT 3 \n",
    "\n",
    "config = {}\n",
    "\n",
    "## define o prompt template\n",
    "prompt_template = templates.PROMPT_TEMPLATE_3\n",
    "config.update({\"prompt_template\": prompt_template})\n",
    "\n",
    "## define o prompt para formatar a resposta final \n",
    "#prompt_format = templates.PROMPT_TEMPLATE_ESTRUCTURE\n",
    "prompt_format = \"\" # para não utilizar \n",
    "config.update({\"prompt_format\": prompt_format})\n",
    "\n",
    "# load movie lens 100k dataset\n",
    "dataset = utils.read_json(\"Data/ML100K_clean.json\")\n",
    "print(f'Quantidade de Usuários: {len(dataset)}')\n",
    "\n",
    "config.update({\n",
    "    #\"runtime\": \"ROCm llama.cpp v1.23.0\", \n",
    "    #\"runtime\": \"CPU llama.cpp v1.22.2\", # performance ruim\n",
    "    \"LLM_runtime\": \"Vulkan llama.cpp v1.23.0\", # melhor opção\n",
    "    \"dataset\": \"ml_100k\",\n",
    "    \"nsu\" : 12,     # número de usuários para filtragem colaborativa        | SSBD :12  | Default :18   | \n",
    "    \"nci\" :19,      # número de itens para filtragem colaborativa           | SSBD :19  | Default :24   | Max : 1682\n",
    "    \"lenlimit\" : 8,  # limite de tamanho para a lista filmes assistidos    | SSBD : 8  | Default :24   | Max : 1682\n",
    "    \"test_run\" : 0, # define a quantidade de recomendações,                 |           | Default :0    | Max : 943 \n",
    "    \"obs\": \"testando prompt novo - prompt3\"\n",
    "})\n",
    "\n",
    "config.update({\n",
    "    \"model_name\" :\"gemma-3-4b-it\",\n",
    "    \"Arch\" : \"gemma3\",\n",
    "    \"Quantization\" : \"Q4_K_M\",\n",
    "    \"Temperature\": 0.1,\n",
    "    \"max_tokens\" : -1,  # Default : 4096\n",
    "    \"GPU Offload\": 34,\n",
    "    \"CPU Thread Pool Size\": 6,\n",
    "    \"Evaluation Batch Size\": 512,\n",
    "    \"Flash Attention\": False, # não vi vantagem no uso \n",
    "})\n",
    "\n",
    "config_list = []\n",
    "\n",
    "config1 = config.copy()\n",
    "config_list.append(config1)\n",
    "\n",
    "for i in range(0, len(config_list)):\n",
    "    config = config_list[i]\n",
    "    print(f'Rodando configuração {i} de {len(config_list)}')\n",
    "    try:\n",
    "        result_pkl = recommender.recommendation_workflow(config         = config,\n",
    "                                                         dataset        = dataset,\n",
    "                                                        prompt_template= prompt_template,\n",
    "                                                        prompt_format  = prompt_format)\n",
    "        print(f'Configuração {i} de {len(config_list)} finalizada')\n",
    "    except Exception as e:\n",
    "        print(f'Erro na configuração {i} de {len(config_list)}')\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(config_list)):\n",
    "    config = config_list[i]\n",
    "    print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{result_pkl}', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for key, value in data.items():\n",
    "    if isinstance(key, int) and isinstance(value, dict):  # Pegando apenas os experimentos\n",
    "        results.append({\n",
    "            'Candidates': value.get('candidate_set', ''),\n",
    "            'Ground Truth': value.get('ground_truth', ''),\n",
    "            'gt_in_candidate_set': value.get('gt_in_candidate_set', ''),\n",
    "            #'Input 1': value.get('input_1', ''),\n",
    "            #'Predictions 1': value.get('predictions_1', ''),\n",
    "            #'Input 2': value.get('input_2', ''),\n",
    "            #'Predictions 2': value.get('predictions_2', ''),\n",
    "            'Input 3': value.get('input_3', ''),\n",
    "            'Predictions 3': value.get('predictions_3', ''),\n",
    "            #'Recommendations': value.get('recommendations', ''),\n",
    "            'rec_HitRate@10': value.get('rec_HitRate@10', ''),\n",
    "            #'Precision': value.get('precision', ''),\n",
    "            #'Recall': value.get('recall', ''),\n",
    "            'rec_NDCG@10': value.get('rec_NDCG@10', '')\n",
    "        })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparar dataset / dividir em treino e teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cria uma lista dos usuários com GT na lista de candidatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from Utils import templates, utils\n",
    "\n",
    "nsu = 12\n",
    "nci = 19\n",
    "lenlimit = 8 \n",
    "\n",
    "dataset = utils.read_json(\"Data/ML100K_clean.json\")\n",
    "movie_idx = utils.build_moviename_index_dict(dataset)\n",
    "user_sim_matrix = utils.build_user_similarity_matrix(dataset, movie_idx)\n",
    "id_list = list(range(0, len(dataset)))\n",
    "\n",
    "data_list = []\n",
    "id_list_com_gt_no_candidate=[]\n",
    "\n",
    "for i in id_list:\n",
    "\n",
    "  watched_mv = dataset[i][0].split(' | ')[::-1]\n",
    "  watched_mv = watched_mv[-lenlimit:]\n",
    "  \n",
    "  groundTruth = dataset[i][-1]\n",
    "  candidate_items = utils.sort_collaborative_user_filtering(target_user_id=i,\n",
    "                                                                    dataset=dataset,\n",
    "                                                                    user_similarity_matrix=user_sim_matrix,\n",
    "                                                                    num_users=nsu,\n",
    "                                                                    num_items=nci,\n",
    "                                                                    include_similar_user_GT=False,\n",
    "                                                                    debug=False)\n",
    "  random.shuffle(candidate_items)\n",
    "\n",
    "  # verifica se o ground_truth está no candidate_set\n",
    "  gt_in_candidate_set = True if any(groundTruth.lower() in candidate.lower() for candidate in candidate_items) else False\n",
    "\n",
    "  if gt_in_candidate_set == True:\n",
    "    id_list_com_gt_no_candidate.append(i)\n",
    "\n",
    "  data_list.append({\n",
    "        'user_id': i,\n",
    "        'watched_movies': watched_mv,\n",
    "        'ground_truth': groundTruth,\n",
    "        'candidate_items': candidate_items,\n",
    "        'gt_in_candidate_set': gt_in_candidate_set\n",
    "  })\n",
    "\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "print(f'nsu: {nsu} \\nnci: {nci} \\nQnt de usuários do dataset: {len(dataset)} \\nQnt de usuários com gt no candidate: {len(id_list_com_gt_no_candidate)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realiza a divisão de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a proporção de divisão\n",
    "train_ratio = 0.8\n",
    "test_ratio = 0.2\n",
    "\n",
    "# Embaralha os IDs para garantir aleatoriedade\n",
    "random.shuffle(id_list_com_gt_no_candidate)\n",
    "\n",
    "# Calcula o tamanho de cada conjunto\n",
    "total_ids = len(id_list_com_gt_no_candidate)\n",
    "train_size = int(train_ratio * total_ids)\n",
    "test_size = total_ids - train_size\n",
    "\n",
    "# Separa os IDs em conjuntos\n",
    "train_ids = id_list_com_gt_no_candidate[:train_size]\n",
    "test_ids = id_list_com_gt_no_candidate[train_size:]\n",
    "\n",
    "# Imprime o tamanho de cada conjunto para verificação\n",
    "print(f\"Tamanho do conjunto de treino: {len(train_ids)}\")\n",
    "print(f\"Tamanho do conjunto de teste: {len(test_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajusta o dataset de treino, adicionando as instruções necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = []\n",
    "\n",
    "# configs \n",
    "model_name = \"gemma-3-4b-it\"\n",
    "temperature = 0\n",
    "prompt_template = templates.PROMPT_TEMPLATE_3\n",
    "system_prompt = \"You are a movie expert provide the answer for the question based on the given context. Your answer must be short \" \n",
    "max_tokens = -1\n",
    "lenlimit = 8\n",
    "\n",
    "for id in train_ids:\n",
    "    filtered = df[df['user_id'] == id]\n",
    "    if not filtered.empty:\n",
    "        candidate_items = filtered['candidate_items'].iloc[0]\n",
    "        ground_truth = filtered['ground_truth'].iloc[0]\n",
    "        watched_movies = filtered['watched_movies'].iloc[0]\n",
    "    else:\n",
    "        continue \n",
    "\n",
    "    input_prompt = prompt_template['Preference'].format(', '.join(candidate_items),', '.join(watched_movies))\n",
    "    response = utils.query_lm_studio(model_name,0.0,system_prompt,input_prompt,max_tokens)\n",
    "    movie_preference = response\n",
    "\n",
    "    input_prompt = prompt_template['Featured_movies'].format(', '.join(watched_mv), movie_preference)\n",
    "    response = utils.query_lm_studio(model_name,0.0,system_prompt,input_prompt,max_tokens)\n",
    "    most_featured = response\n",
    "\n",
    "    input = templates.TRAIN_TEMPLATE_2['INPUT'].format(', '.join(candidate_items),', '.join(watched_movies),movie_preference,most_featured )\n",
    "    output= templates.TRAIN_TEMPLATE_2['OUTPUT'].format(ground_truth)\n",
    "\n",
    "    train_dataset.append({\n",
    "        'input': input,\n",
    "        'output': output\n",
    "  })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(train_dataset)\n",
    "df_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salva arquivo de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_pickle('Data/ML100K_train.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salva arquivo de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = []\n",
    "\n",
    "for id in test_ids:\n",
    "    filtered = df[df['user_id'] == id]\n",
    "    if not filtered.empty:\n",
    "        candidate_items = filtered['candidate_items'].iloc[0]\n",
    "        ground_truth = filtered['ground_truth'].iloc[0]\n",
    "        watched_movies = filtered['watched_movies'].iloc[0]\n",
    "    else:\n",
    "        continue \n",
    "    \n",
    "    test_dataset.append({\n",
    "        'user_id': id,\n",
    "        'candidate_items': candidate_items,\n",
    "        'ground_truth': ground_truth,\n",
    "        'watched_movies': watched_movies\n",
    "    })\n",
    "df_test = pd.DataFrame(test_dataset)\n",
    "df_test.to_pickle('Data/ML100K_test.pkl')\n",
    "df_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
