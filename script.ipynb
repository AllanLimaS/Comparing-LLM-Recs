{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuração inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from Utils import templates, utils, recommender, models_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de Usuários: 6040\n"
     ]
    }
   ],
   "source": [
    "## Configurações base\n",
    "\n",
    "config = {\n",
    "    #\"runtime\": \"ROCm llama.cpp v1.23.0\", \n",
    "    #\"runtime\": \"CPU llama.cpp v1.22.2\", # performance ruim\n",
    "    \"LLM_runtime\": \"Vulkan llama.cpp v1.23.0\", # melhor opção\n",
    "    \"dataset\": \"ml_1m\",                                                  #| Opções 'ml_100k' e 'ml_1m'\n",
    "    \"nsu\" : 12,     # número de usuários para filtragem colaborativa        | SSBD :12  | Default :18   |            | Best = 19\n",
    "    \"nci\" :19,      # número de itens para filtragem colaborativa           | SSBD :19  | Default :24   | Max : 1682 |\n",
    "    \"lenlimit\" : 8,  # limite de tamanho para a lista filmes assistidos     | SSBD : 8  | Default :24   | Max : 1682 | Best = 8\n",
    "    \"lenlimit_option\" : 'ultimos', # define qual a abordagem                | opções : 'ultimos', 'primeiros', 'aleatorio' | Default : 'ultimos'\n",
    "    \"test_run\" : 0, # define a quantidade de recomendações,                 |           | Default :0    | Max : 943 \n",
    "    \"obs\": \"testando dataset\"\n",
    "}\n",
    "\n",
    "## define o prompt template\n",
    "prompt_template = templates.PROMPT_TEMPLATE_3\n",
    "config.update({\"prompt_template\": prompt_template})\n",
    "\n",
    "## define o prompt para formatar a resposta final \n",
    "#prompt_format = templates.PROMPT_TEMPLATE_ESTRUCTURE\n",
    "prompt_format = \"\" # para não utilizar \n",
    "config.update({\"prompt_format\": prompt_format})\n",
    "\n",
    "# load movie lens 100k dataset\n",
    "dataset = utils.read_json(f\"Data/{config['dataset']}.json\")\n",
    "print(f'Quantidade de Usuários: {len(dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.update(models_config.qwen_2_5_7b_instruct_1m())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execução unitária"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pkl = recommender.recommendation_workflow(config         = config,\n",
    "                                                 dataset        = dataset,\n",
    "                                                 prompt_template= prompt_template,\n",
    "                                                 prompt_format  = prompt_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execução multipla "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LLM_runtime': 'Vulkan llama.cpp v1.23.0', 'dataset': 'ml_100k', 'nsu': 12, 'nci': 19, 'lenlimit': 8, 'lenlimit_option': 'ultimos', 'test_run': 0, 'obs': 'estando dataset', 'prompt_template': {'System_prompt': \"You are a movie expert provide the answer for the question based on the given context. If you don't know the answer to a question, please don't share false information.\", 'Preference': '\\n    ### MY RECENTLY WATCHED MOVIES (FROM OLDEST TO NEWEST): {}.\\n\\n    ### QUESTION: Based on my watched movies list. Tell me what features are most important to me when selecting movies (Summarize my preferences briefly)?\\n\\n    ### ANSWER:\\n    ', 'Featured_movies': '\\n\\n    ### MY RECENTLY WATCHED MOVIES (FROM OLDEST TO NEWEST): {}.\\n\\n    ### MY MOVIE PREFERENCES: {}.\\n\\n    ### QUESTION: Create an enumerated list selecting the five most featured movies from the watched movies according to my movie preferences.\\n\\n    ### ANSWER:\\n    ', 'Recommendation': '\\n\\n    ### CANDIDATE MOVIE SET: {}.\\n\\n    ### MY RECENTLY WATCHED MOVIES (FROM OLDEST TO NEWEST): {}.\\n\\n    ### MY MOVIE PREFERENCES: {}.\\n\\n    ### MY FIVE MOST FEATURED MOVIES: {}.\\n\\n    ### INSTRUCTIONS:\\n    Recommend exactly **10 movies** from the \"Candidate Movie Set\" that are most similar to my \"Five Most Featured Movies\".  \\n    The output must be **STRICTLY formatted** as follows:\\n\\n    1. Movie Title  \\n    2. Movie Title  \\n    3. Movie Title  \\n    4. Movie Title  \\n    5. Movie Title  \\n    6. Movie Title  \\n    7. Movie Title  \\n    8. Movie Title  \\n    9. Movie Title  \\n    10. Movie Title  \\n\\n    Do not include any extra text before or after the list \\n    \\n    ### QUESTION:  \\n    What are your top 10 movie recommendations?  \\n\\n    ### ANSWER:\\n'}, 'prompt_format': '', 'model_name': 'qwen2.5-7b-instruct-1m', 'Arch': 'qwen2', 'Quantization': 'Q4_K_M', 'Temperature': 0.0, 'max_tokens': -1, 'GPU Offload': 28, 'CPU Thread Pool Size': 6, 'Evaluation Batch Size': 512, 'Flash Attention': False}\n",
      "{'LLM_runtime': 'Vulkan llama.cpp v1.23.0', 'dataset': 'ml_100k', 'nsu': 12, 'nci': 19, 'lenlimit': 8, 'lenlimit_option': 'ultimos', 'test_run': 0, 'obs': 'estando dataset', 'prompt_template': {'System_prompt': \"You are a movie expert provide the answer for the question based on the given context. If you don't know the answer to a question, please don't share false information.\", 'Preference': '\\n    ### MY RECENTLY WATCHED MOVIES (FROM OLDEST TO NEWEST): {}.\\n\\n    ### QUESTION: Based on my watched movies list. Tell me what features are most important to me when selecting movies (Summarize my preferences briefly)?\\n\\n    ### ANSWER:\\n    ', 'Featured_movies': '\\n\\n    ### MY RECENTLY WATCHED MOVIES (FROM OLDEST TO NEWEST): {}.\\n\\n    ### MY MOVIE PREFERENCES: {}.\\n\\n    ### QUESTION: Create an enumerated list selecting the five most featured movies from the watched movies according to my movie preferences.\\n\\n    ### ANSWER:\\n    ', 'Recommendation': '\\n\\n    ### CANDIDATE MOVIE SET: {}.\\n\\n    ### MY RECENTLY WATCHED MOVIES (FROM OLDEST TO NEWEST): {}.\\n\\n    ### MY MOVIE PREFERENCES: {}.\\n\\n    ### MY FIVE MOST FEATURED MOVIES: {}.\\n\\n    ### INSTRUCTIONS:\\n    Recommend exactly **10 movies** from the \"Candidate Movie Set\" that are most similar to my \"Five Most Featured Movies\".  \\n    The output must be **STRICTLY formatted** as follows:\\n\\n    1. Movie Title  \\n    2. Movie Title  \\n    3. Movie Title  \\n    4. Movie Title  \\n    5. Movie Title  \\n    6. Movie Title  \\n    7. Movie Title  \\n    8. Movie Title  \\n    9. Movie Title  \\n    10. Movie Title  \\n\\n    Do not include any extra text before or after the list \\n    \\n    ### QUESTION:  \\n    What are your top 10 movie recommendations?  \\n\\n    ### ANSWER:\\n'}, 'prompt_format': '', 'model_name': 'qwen2.5-7b-instruct-1m', 'Arch': 'qwen2', 'Quantization': 'Q4_K_M', 'Temperature': 0.0, 'max_tokens': -1, 'GPU Offload': 28, 'CPU Thread Pool Size': 6, 'Evaluation Batch Size': 512, 'Flash Attention': False}\n",
      "{'LLM_runtime': 'Vulkan llama.cpp v1.23.0', 'dataset': 'ml_1m', 'nsu': 12, 'nci': 19, 'lenlimit': 8, 'lenlimit_option': 'ultimos', 'test_run': 0, 'obs': 'estando dataset', 'prompt_template': {'System_prompt': \"You are a movie expert provide the answer for the question based on the given context. If you don't know the answer to a question, please don't share false information.\", 'Preference': '\\n    ### MY RECENTLY WATCHED MOVIES (FROM OLDEST TO NEWEST): {}.\\n\\n    ### QUESTION: Based on my watched movies list. Tell me what features are most important to me when selecting movies (Summarize my preferences briefly)?\\n\\n    ### ANSWER:\\n    ', 'Featured_movies': '\\n\\n    ### MY RECENTLY WATCHED MOVIES (FROM OLDEST TO NEWEST): {}.\\n\\n    ### MY MOVIE PREFERENCES: {}.\\n\\n    ### QUESTION: Create an enumerated list selecting the five most featured movies from the watched movies according to my movie preferences.\\n\\n    ### ANSWER:\\n    ', 'Recommendation': '\\n\\n    ### CANDIDATE MOVIE SET: {}.\\n\\n    ### MY RECENTLY WATCHED MOVIES (FROM OLDEST TO NEWEST): {}.\\n\\n    ### MY MOVIE PREFERENCES: {}.\\n\\n    ### MY FIVE MOST FEATURED MOVIES: {}.\\n\\n    ### INSTRUCTIONS:\\n    Recommend exactly **10 movies** from the \"Candidate Movie Set\" that are most similar to my \"Five Most Featured Movies\".  \\n    The output must be **STRICTLY formatted** as follows:\\n\\n    1. Movie Title  \\n    2. Movie Title  \\n    3. Movie Title  \\n    4. Movie Title  \\n    5. Movie Title  \\n    6. Movie Title  \\n    7. Movie Title  \\n    8. Movie Title  \\n    9. Movie Title  \\n    10. Movie Title  \\n\\n    Do not include any extra text before or after the list \\n    \\n    ### QUESTION:  \\n    What are your top 10 movie recommendations?  \\n\\n    ### ANSWER:\\n'}, 'prompt_format': '', 'model_name': 'gemma-3-4b-it', 'Arch': 'gemma3', 'Quantization': 'Q4_K_M', 'Temperature': 0.1, 'max_tokens': -1, 'GPU Offload': 34, 'CPU Thread Pool Size': 6, 'Evaluation Batch Size': 512, 'Flash Attention': False}\n",
      "{'LLM_runtime': 'Vulkan llama.cpp v1.23.0', 'dataset': 'ml_1m', 'nsu': 12, 'nci': 19, 'lenlimit': 8, 'lenlimit_option': 'ultimos', 'test_run': 0, 'obs': 'estando dataset', 'prompt_template': {'System_prompt': \"You are a movie expert provide the answer for the question based on the given context. If you don't know the answer to a question, please don't share false information.\", 'Preference': '\\n    ### MY RECENTLY WATCHED MOVIES (FROM OLDEST TO NEWEST): {}.\\n\\n    ### QUESTION: Based on my watched movies list. Tell me what features are most important to me when selecting movies (Summarize my preferences briefly)?\\n\\n    ### ANSWER:\\n    ', 'Featured_movies': '\\n\\n    ### MY RECENTLY WATCHED MOVIES (FROM OLDEST TO NEWEST): {}.\\n\\n    ### MY MOVIE PREFERENCES: {}.\\n\\n    ### QUESTION: Create an enumerated list selecting the five most featured movies from the watched movies according to my movie preferences.\\n\\n    ### ANSWER:\\n    ', 'Recommendation': '\\n\\n    ### CANDIDATE MOVIE SET: {}.\\n\\n    ### MY RECENTLY WATCHED MOVIES (FROM OLDEST TO NEWEST): {}.\\n\\n    ### MY MOVIE PREFERENCES: {}.\\n\\n    ### MY FIVE MOST FEATURED MOVIES: {}.\\n\\n    ### INSTRUCTIONS:\\n    Recommend exactly **10 movies** from the \"Candidate Movie Set\" that are most similar to my \"Five Most Featured Movies\".  \\n    The output must be **STRICTLY formatted** as follows:\\n\\n    1. Movie Title  \\n    2. Movie Title  \\n    3. Movie Title  \\n    4. Movie Title  \\n    5. Movie Title  \\n    6. Movie Title  \\n    7. Movie Title  \\n    8. Movie Title  \\n    9. Movie Title  \\n    10. Movie Title  \\n\\n    Do not include any extra text before or after the list \\n    \\n    ### QUESTION:  \\n    What are your top 10 movie recommendations?  \\n\\n    ### ANSWER:\\n'}, 'prompt_format': '', 'model_name': 'gemma-3-4b-it', 'Arch': 'gemma3', 'Quantization': 'Q4_K_M', 'Temperature': 0.1, 'max_tokens': -1, 'GPU Offload': 34, 'CPU Thread Pool Size': 6, 'Evaluation Batch Size': 512, 'Flash Attention': False}\n",
      "{'LLM_runtime': 'Vulkan llama.cpp v1.23.0', 'dataset': 'ml_100k', 'nsu': 12, 'nci': 19, 'lenlimit': 8, 'lenlimit_option': 'ultimos', 'test_run': 0, 'obs': 'estando dataset', 'prompt_template': {'System_prompt': \"You are a movie expert provide the answer for the question based on the given context. If you don't know the answer to a question, please don't share false information.\", 'Preference': '\\n    ### MY RECENTLY WATCHED MOVIES (FROM OLDEST TO NEWEST): {}.\\n\\n    ### QUESTION: Based on my watched movies list. Tell me what features are most important to me when selecting movies (Summarize my preferences briefly)?\\n\\n    ### ANSWER:\\n    ', 'Featured_movies': '\\n\\n    ### MY RECENTLY WATCHED MOVIES (FROM OLDEST TO NEWEST): {}.\\n\\n    ### MY MOVIE PREFERENCES: {}.\\n\\n    ### QUESTION: Create an enumerated list selecting the five most featured movies from the watched movies according to my movie preferences.\\n\\n    ### ANSWER:\\n    ', 'Recommendation': '\\n\\n    ### CANDIDATE MOVIE SET: {}.\\n\\n    ### MY RECENTLY WATCHED MOVIES (FROM OLDEST TO NEWEST): {}.\\n\\n    ### MY MOVIE PREFERENCES: {}.\\n\\n    ### MY FIVE MOST FEATURED MOVIES: {}.\\n\\n    ### INSTRUCTIONS:\\n    Recommend exactly **10 movies** from the \"Candidate Movie Set\" that are most similar to my \"Five Most Featured Movies\".  \\n    The output must be **STRICTLY formatted** as follows:\\n\\n    1. Movie Title  \\n    2. Movie Title  \\n    3. Movie Title  \\n    4. Movie Title  \\n    5. Movie Title  \\n    6. Movie Title  \\n    7. Movie Title  \\n    8. Movie Title  \\n    9. Movie Title  \\n    10. Movie Title  \\n\\n    Do not include any extra text before or after the list \\n    \\n    ### QUESTION:  \\n    What are your top 10 movie recommendations?  \\n\\n    ### ANSWER:\\n'}, 'prompt_format': '', 'model_name': 'gemma-3-4b-it', 'Arch': 'gemma3', 'Quantization': 'Q4_K_M', 'Temperature': 0.1, 'max_tokens': -1, 'GPU Offload': 34, 'CPU Thread Pool Size': 6, 'Evaluation Batch Size': 512, 'Flash Attention': False}\n",
      "{'LLM_runtime': 'Vulkan llama.cpp v1.23.0', 'dataset': 'ml_100k', 'nsu': 12, 'nci': 19, 'lenlimit': 8, 'lenlimit_option': 'ultimos', 'test_run': 0, 'obs': 'estando dataset', 'prompt_template': {'System_prompt': \"You are a movie expert provide the answer for the question based on the given context. If you don't know the answer to a question, please don't share false information.\", 'Preference': '\\n    ### MY RECENTLY WATCHED MOVIES (FROM OLDEST TO NEWEST): {}.\\n\\n    ### QUESTION: Based on my watched movies list. Tell me what features are most important to me when selecting movies (Summarize my preferences briefly)?\\n\\n    ### ANSWER:\\n    ', 'Featured_movies': '\\n\\n    ### MY RECENTLY WATCHED MOVIES (FROM OLDEST TO NEWEST): {}.\\n\\n    ### MY MOVIE PREFERENCES: {}.\\n\\n    ### QUESTION: Create an enumerated list selecting the five most featured movies from the watched movies according to my movie preferences.\\n\\n    ### ANSWER:\\n    ', 'Recommendation': '\\n\\n    ### CANDIDATE MOVIE SET: {}.\\n\\n    ### MY RECENTLY WATCHED MOVIES (FROM OLDEST TO NEWEST): {}.\\n\\n    ### MY MOVIE PREFERENCES: {}.\\n\\n    ### MY FIVE MOST FEATURED MOVIES: {}.\\n\\n    ### INSTRUCTIONS:\\n    Recommend exactly **10 movies** from the \"Candidate Movie Set\" that are most similar to my \"Five Most Featured Movies\".  \\n    The output must be **STRICTLY formatted** as follows:\\n\\n    1. Movie Title  \\n    2. Movie Title  \\n    3. Movie Title  \\n    4. Movie Title  \\n    5. Movie Title  \\n    6. Movie Title  \\n    7. Movie Title  \\n    8. Movie Title  \\n    9. Movie Title  \\n    10. Movie Title  \\n\\n    Do not include any extra text before or after the list \\n    \\n    ### QUESTION:  \\n    What are your top 10 movie recommendations?  \\n\\n    ### ANSWER:\\n'}, 'prompt_format': '', 'model_name': 'gemma-3-4b-it', 'Arch': 'gemma3', 'Quantization': 'Q4_K_M', 'Temperature': 0.1, 'max_tokens': -1, 'GPU Offload': 34, 'CPU Thread Pool Size': 6, 'Evaluation Batch Size': 512, 'Flash Attention': False}\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(config_list)):\n",
    "    config = config_list[i]\n",
    "    print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rodando configuração 0 de 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b028d5d35f08468c9dce1f252105e082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processando:   0%|          | 0/943 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuração 0 de 6 finalizada\n",
      "Rodando configuração 1 de 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cafcc2d36187498384b25a82fc5fb727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processando:   0%|          | 0/943 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuração 1 de 6 finalizada\n",
      "Rodando configuração 2 de 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c909e72922f4ce090c8180a0e0b7a9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processando:   0%|          | 0/6040 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tentativa 1/2] Modelo ainda não carregado? Erro 400. Aguardando 60s...\n",
      "Configuração 2 de 6 finalizada\n",
      "Rodando configuração 3 de 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a95f33a689444eff97c1eb7eff6f3a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processando:   0%|          | 0/6040 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuração 3 de 6 finalizada\n",
      "Rodando configuração 4 de 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f693ed4ab8c04ef0bd8bacac9d1e7b81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processando:   0%|          | 0/943 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuração 4 de 6 finalizada\n",
      "Rodando configuração 5 de 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "defda452ac934ca48635fd01f5b1b77d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processando:   0%|          | 0/943 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuração 5 de 6 finalizada\n"
     ]
    }
   ],
   "source": [
    "config_list = []\n",
    "\n",
    "config.update({\"dataset\": \"ml_100k\"})\n",
    "config.update({\"obs\": \"estando dataset\"})\n",
    "config.update(models_config.qwen_2_5_7b_instruct_1m())\n",
    "\n",
    "config1 = config.copy()\n",
    "config_list.append(config1)\n",
    "\n",
    "config2 = config1.copy()\n",
    "config_list.append(config2)\n",
    "\n",
    "config3 = config.copy()\n",
    "config3.update({\"dataset\": \"ml_1m\"})\n",
    "config3.update(models_config.gemma_3_4b_it())\n",
    "config_list.append(config3)\n",
    "\n",
    "config4 = config3.copy()\n",
    "config_list.append(config4)\n",
    "\n",
    "config5 = config.copy()\n",
    "config5.update({\"dataset\": \"ml_100k\"})\n",
    "config5.update(models_config.gemma_3_4b_it())\n",
    "config_list.append(config5)\n",
    "\n",
    "config6 = config5.copy()\n",
    "config_list.append(config6)\n",
    "\n",
    "for i in range(0, len(config_list)):\n",
    "    config = config_list[i]\n",
    "    dataset = utils.read_json(f\"Data/{config['dataset']}.json\")\n",
    "    print(f'Rodando configuração {i} de {len(config_list)}')\n",
    "    try:\n",
    "        result_pkl = recommender.recommendation_workflow(config         = config,\n",
    "                                                         dataset        = dataset,\n",
    "                                                        prompt_template= prompt_template,\n",
    "                                                        prompt_format  = prompt_format)\n",
    "        print(f'Configuração {i} de {len(config_list)} finalizada')\n",
    "    except Exception as e:\n",
    "        print(f'Erro na configuração {i} de {len(config_list)}')\n",
    "        print(e)\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# outros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retorna apenas o usuários que tem o gt no candidatos.\n",
    "# implementar sobre o pipeline do projeto \n",
    "\n",
    "def get_candidate_ids_list(data, id_list, user_matrix_sim, num_u, num_i):\n",
    "    cand_ids = []\n",
    "    for i in id_list:\n",
    "        watched_movies = data[i][0].split(' | ')\n",
    "        candidate_items = utils.sort_user_filtering_items(data, watched_movies, user_matrix_sim[i], num_u, num_i)\n",
    "        if data[i][-1] in candidate_items:\n",
    "            cand_ids.append(i)\n",
    "    return cand_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = list(range(0, len(dataset)))\n",
    "#assert(len(id_list) == 943) # aqui é verificado se a lista possue exatamente essa quantidade\n",
    "\n",
    "# Building indexes and similarity matrices for users and movies.\n",
    "movie_idx = utils.build_moviename_index_dict(dataset)\n",
    "user_sim_matrix = utils.build_user_similarity_matrix(dataset, movie_idx)\n",
    "\n",
    "\n",
    "# para fazer a filtragem sobre os filmes \n",
    "#pop_dict = utils.build_movie_popularity_dict(dataset) \n",
    "#item_sim_matrix = utils.build_item_similarity_matrix(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pkl = recommender.recommendation_workflow(config         = config,\n",
    "                                                 dataset        = dataset,\n",
    "                                                 prompt_template= prompt_template,\n",
    "                                                 prompt_format  = prompt_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## execução varias configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from Utils import templates, utils, recommender\n",
    "\n",
    "## PROMPT 2 \n",
    "\n",
    "config = {}\n",
    "\n",
    "## define o prompt template\n",
    "prompt_template = templates.PROMPT_TEMPLATE_2\n",
    "config.update({\"prompt_template\": prompt_template})\n",
    "\n",
    "## define o prompt para formatar a resposta final \n",
    "#prompt_format = templates.PROMPT_TEMPLATE_ESTRUCTURE\n",
    "prompt_format = \"\" # para não utilizar \n",
    "config.update({\"prompt_format\": prompt_format})\n",
    "\n",
    "# load movie lens 100k dataset\n",
    "dataset = utils.read_json(\"Data/ML100K_clean.json\")\n",
    "print(f'Quantidade de Usuários: {len(dataset)}')\n",
    "\n",
    "config.update({\n",
    "    #\"runtime\": \"ROCm llama.cpp v1.23.0\", \n",
    "    #\"runtime\": \"CPU llama.cpp v1.22.2\", # performance ruim\n",
    "    \"LLM_runtime\": \"Vulkan llama.cpp v1.23.0\", # melhor opção\n",
    "    \"dataset\": \"ml_100k\",\n",
    "    \"nsu\" : 19,     # número de usuários para filtragem colaborativa        | SSBD :12  | Default :18   | \n",
    "    \"nci\" :19,      # número de itens para filtragem colaborativa           | SSBD :19  | Default :24   | Max : 1682\n",
    "    \"lenlimit\" : 8,  # limite de tamanho para a lista filmes assistidos    | SSBD : 8  | Default :24   | Max : 1682\n",
    "    \"test_run\" : 0, # define a quantidade de recomendações,                 |           | Default :0    | Max : 943 \n",
    "    \"obs\": \"testando prompt novo - prompt2\"\n",
    "})\n",
    "\n",
    "config.update({\n",
    "    \"model_name\" :\"gemma-3-4b-it\",\n",
    "    \"Arch\" : \"gemma3\",\n",
    "    \"Quantization\" : \"Q4_K_M\",\n",
    "    \"Temperature\": 0.1,\n",
    "    \"max_tokens\" : -1,  # Default : 4096\n",
    "    \"GPU Offload\": 34,\n",
    "    \"CPU Thread Pool Size\": 6,\n",
    "    \"Evaluation Batch Size\": 512,\n",
    "    \"Flash Attention\": False, # não vi vantagem no uso \n",
    "})\n",
    "\n",
    "config_list = []\n",
    "\n",
    "config1 = config.copy()\n",
    "config_list.append(config1)\n",
    "\n",
    "for i in range(0, len(config_list)):\n",
    "    config = config_list[i]\n",
    "    print(f'Rodando configuração {i} de {len(config_list)}')\n",
    "    try:\n",
    "        result_pkl = recommender.recommendation_workflow(config         = config,\n",
    "                                                         dataset        = dataset,\n",
    "                                                        prompt_template= prompt_template,\n",
    "                                                        prompt_format  = prompt_format)\n",
    "        print(f'Configuração {i} de {len(config_list)} finalizada')\n",
    "    except Exception as e:\n",
    "        print(f'Erro na configuração {i} de {len(config_list)}')\n",
    "        print(e)\n",
    "        continue\n",
    "\n",
    "## PROMPT 3 \n",
    "\n",
    "config = {}\n",
    "\n",
    "## define o prompt template\n",
    "prompt_template = templates.PROMPT_TEMPLATE_3\n",
    "config.update({\"prompt_template\": prompt_template})\n",
    "\n",
    "## define o prompt para formatar a resposta final \n",
    "#prompt_format = templates.PROMPT_TEMPLATE_ESTRUCTURE\n",
    "prompt_format = \"\" # para não utilizar \n",
    "config.update({\"prompt_format\": prompt_format})\n",
    "\n",
    "# load movie lens 100k dataset\n",
    "dataset = utils.read_json(\"Data/ML100K_clean.json\")\n",
    "print(f'Quantidade de Usuários: {len(dataset)}')\n",
    "\n",
    "config.update({\n",
    "    #\"runtime\": \"ROCm llama.cpp v1.23.0\", \n",
    "    #\"runtime\": \"CPU llama.cpp v1.22.2\", # performance ruim\n",
    "    \"LLM_runtime\": \"Vulkan llama.cpp v1.23.0\", # melhor opção\n",
    "    \"dataset\": \"ml_100k\",\n",
    "    \"nsu\" : 12,     # número de usuários para filtragem colaborativa        | SSBD :12  | Default :18   | \n",
    "    \"nci\" :19,      # número de itens para filtragem colaborativa           | SSBD :19  | Default :24   | Max : 1682\n",
    "    \"lenlimit\" : 8,  # limite de tamanho para a lista filmes assistidos    | SSBD : 8  | Default :24   | Max : 1682\n",
    "    \"test_run\" : 0, # define a quantidade de recomendações,                 |           | Default :0    | Max : 943 \n",
    "    \"obs\": \"testando prompt novo - prompt3\"\n",
    "})\n",
    "\n",
    "config.update({\n",
    "    \"model_name\" :\"gemma-3-4b-it\",\n",
    "    \"Arch\" : \"gemma3\",\n",
    "    \"Quantization\" : \"Q4_K_M\",\n",
    "    \"Temperature\": 0.1,\n",
    "    \"max_tokens\" : -1,  # Default : 4096\n",
    "    \"GPU Offload\": 34,\n",
    "    \"CPU Thread Pool Size\": 6,\n",
    "    \"Evaluation Batch Size\": 512,\n",
    "    \"Flash Attention\": False, # não vi vantagem no uso \n",
    "})\n",
    "\n",
    "config_list = []\n",
    "\n",
    "config1 = config.copy()\n",
    "config_list.append(config1)\n",
    "\n",
    "for i in range(0, len(config_list)):\n",
    "    config = config_list[i]\n",
    "    print(f'Rodando configuração {i} de {len(config_list)}')\n",
    "    try:\n",
    "        result_pkl = recommender.recommendation_workflow(config         = config,\n",
    "                                                         dataset        = dataset,\n",
    "                                                        prompt_template= prompt_template,\n",
    "                                                        prompt_format  = prompt_format)\n",
    "        print(f'Configuração {i} de {len(config_list)} finalizada')\n",
    "    except Exception as e:\n",
    "        print(f'Erro na configuração {i} de {len(config_list)}')\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(config_list)):\n",
    "    config = config_list[i]\n",
    "    print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{result_pkl}', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for key, value in data.items():\n",
    "    if isinstance(key, int) and isinstance(value, dict):  # Pegando apenas os experimentos\n",
    "        results.append({\n",
    "            'Candidates': value.get('candidate_set', ''),\n",
    "            'Ground Truth': value.get('ground_truth', ''),\n",
    "            'gt_in_candidate_set': value.get('gt_in_candidate_set', ''),\n",
    "            #'Input 1': value.get('input_1', ''),\n",
    "            #'Predictions 1': value.get('predictions_1', ''),\n",
    "            #'Input 2': value.get('input_2', ''),\n",
    "            #'Predictions 2': value.get('predictions_2', ''),\n",
    "            'Input 3': value.get('input_3', ''),\n",
    "            'Predictions 3': value.get('predictions_3', ''),\n",
    "            #'Recommendations': value.get('recommendations', ''),\n",
    "            'rec_HitRate@10': value.get('rec_HitRate@10', ''),\n",
    "            #'Precision': value.get('precision', ''),\n",
    "            #'Recall': value.get('recall', ''),\n",
    "            'rec_NDCG@10': value.get('rec_NDCG@10', '')\n",
    "        })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparar dataset / dividir em treino e teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cria uma lista dos usuários com GT na lista de candidatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from Utils import templates, utils\n",
    "\n",
    "nsu = 12\n",
    "nci = 19\n",
    "lenlimit = 8 \n",
    "\n",
    "dataset = utils.read_json(\"Data/ML100K_clean.json\")\n",
    "movie_idx = utils.build_moviename_index_dict(dataset)\n",
    "user_sim_matrix = utils.build_user_similarity_matrix(dataset, movie_idx)\n",
    "id_list = list(range(0, len(dataset)))\n",
    "\n",
    "data_list = []\n",
    "id_list_com_gt_no_candidate=[]\n",
    "\n",
    "for i in id_list:\n",
    "\n",
    "  watched_mv = dataset[i][0].split(' | ')[::-1]\n",
    "  watched_mv = watched_mv[-lenlimit:]\n",
    "  \n",
    "  groundTruth = dataset[i][-1]\n",
    "  candidate_items = utils.sort_collaborative_user_filtering(target_user_id=i,\n",
    "                                                                    dataset=dataset,\n",
    "                                                                    user_similarity_matrix=user_sim_matrix,\n",
    "                                                                    num_users=nsu,\n",
    "                                                                    num_items=nci,\n",
    "                                                                    include_similar_user_GT=False,\n",
    "                                                                    debug=False)\n",
    "  random.shuffle(candidate_items)\n",
    "\n",
    "  # verifica se o ground_truth está no candidate_set\n",
    "  gt_in_candidate_set = True if any(groundTruth.lower() in candidate.lower() for candidate in candidate_items) else False\n",
    "\n",
    "  if gt_in_candidate_set == True:\n",
    "    id_list_com_gt_no_candidate.append(i)\n",
    "\n",
    "  data_list.append({\n",
    "        'user_id': i,\n",
    "        'watched_movies': watched_mv,\n",
    "        'ground_truth': groundTruth,\n",
    "        'candidate_items': candidate_items,\n",
    "        'gt_in_candidate_set': gt_in_candidate_set\n",
    "  })\n",
    "\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "print(f'nsu: {nsu} \\nnci: {nci} \\nQnt de usuários do dataset: {len(dataset)} \\nQnt de usuários com gt no candidate: {len(id_list_com_gt_no_candidate)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realiza a divisão de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a proporção de divisão\n",
    "train_ratio = 0.8\n",
    "test_ratio = 0.2\n",
    "\n",
    "# Embaralha os IDs para garantir aleatoriedade\n",
    "random.shuffle(id_list_com_gt_no_candidate)\n",
    "\n",
    "# Calcula o tamanho de cada conjunto\n",
    "total_ids = len(id_list_com_gt_no_candidate)\n",
    "train_size = int(train_ratio * total_ids)\n",
    "test_size = total_ids - train_size\n",
    "\n",
    "# Separa os IDs em conjuntos\n",
    "train_ids = id_list_com_gt_no_candidate[:train_size]\n",
    "test_ids = id_list_com_gt_no_candidate[train_size:]\n",
    "\n",
    "# Imprime o tamanho de cada conjunto para verificação\n",
    "print(f\"Tamanho do conjunto de treino: {len(train_ids)}\")\n",
    "print(f\"Tamanho do conjunto de teste: {len(test_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajusta o dataset de treino, adicionando as instruções necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = []\n",
    "\n",
    "# configs \n",
    "model_name = \"gemma-3-4b-it\"\n",
    "temperature = 0\n",
    "prompt_template = templates.PROMPT_TEMPLATE_3\n",
    "system_prompt = \"You are a movie expert provide the answer for the question based on the given context. Your answer must be short \" \n",
    "max_tokens = -1\n",
    "lenlimit = 8\n",
    "\n",
    "for id in train_ids:\n",
    "    filtered = df[df['user_id'] == id]\n",
    "    if not filtered.empty:\n",
    "        candidate_items = filtered['candidate_items'].iloc[0]\n",
    "        ground_truth = filtered['ground_truth'].iloc[0]\n",
    "        watched_movies = filtered['watched_movies'].iloc[0]\n",
    "    else:\n",
    "        continue \n",
    "\n",
    "    input_prompt = prompt_template['Preference'].format(', '.join(candidate_items),', '.join(watched_movies))\n",
    "    response = utils.query_lm_studio(model_name,0.0,system_prompt,input_prompt,max_tokens)\n",
    "    movie_preference = response\n",
    "\n",
    "    input_prompt = prompt_template['Featured_movies'].format(', '.join(watched_mv), movie_preference)\n",
    "    response = utils.query_lm_studio(model_name,0.0,system_prompt,input_prompt,max_tokens)\n",
    "    most_featured = response\n",
    "\n",
    "    input = templates.TRAIN_TEMPLATE_2['INPUT'].format(', '.join(candidate_items),', '.join(watched_movies),movie_preference,most_featured )\n",
    "    output= templates.TRAIN_TEMPLATE_2['OUTPUT'].format(ground_truth)\n",
    "\n",
    "    train_dataset.append({\n",
    "        'input': input,\n",
    "        'output': output\n",
    "  })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(train_dataset)\n",
    "df_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salva arquivo de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_pickle('Data/ML100K_train.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salva arquivo de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = []\n",
    "\n",
    "for id in test_ids:\n",
    "    filtered = df[df['user_id'] == id]\n",
    "    if not filtered.empty:\n",
    "        candidate_items = filtered['candidate_items'].iloc[0]\n",
    "        ground_truth = filtered['ground_truth'].iloc[0]\n",
    "        watched_movies = filtered['watched_movies'].iloc[0]\n",
    "    else:\n",
    "        continue \n",
    "    \n",
    "    test_dataset.append({\n",
    "        'user_id': id,\n",
    "        'candidate_items': candidate_items,\n",
    "        'ground_truth': ground_truth,\n",
    "        'watched_movies': watched_movies\n",
    "    })\n",
    "df_test = pd.DataFrame(test_dataset)\n",
    "df_test.to_pickle('Data/ML100K_test.pkl')\n",
    "df_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
