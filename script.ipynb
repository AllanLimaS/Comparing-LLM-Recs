{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuração inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from Utils import templates, utils, recommender, models_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de Usuários: 943\n"
     ]
    }
   ],
   "source": [
    "## Configurações base\n",
    "\n",
    "config = {\n",
    "    \"LLM_runtime\": \"ROCm llama.cpp v1.29.0\", # melhor opção no momento (ultimo teste: 03/05/2025)\n",
    "    #\"LLM_runtime\": \"CPU llama.cpp v1.22.2\", # performance ruim\n",
    "    #\"LLM_runtime\": \"Vulkan llama.cpp v1.28.0\", # performance ruim\n",
    "    \"dataset\": \"ml_100k\",                                                  #| Opções 'ml_100k' e 'ml_1m'\n",
    "    \"nsu\" : 12,     # número de usuários para filtragem colaborativa        | SSBD :12  | Default :18   |            | Best = 19\n",
    "    \"nci\" :19,      # número de itens para filtragem colaborativa           | SSBD :19  | Default :24   | Max : 1682 |\n",
    "    \"lenlimit\" : 8,  # limite de tamanho para a lista filmes assistidos     | SSBD : 8  | Default :24   | Max : 1682 | Best = 8\n",
    "    \"lenlimit_option\" : 'aleatorio', # define qual a abordagem                | opções : 'ultimos', 'primeiros', 'aleatorio' | Default : 'aleatorio'\n",
    "    \"test_run\" : 0, # define a quantidade de recomendações,                 |           | Default :0    | Max : 943 \n",
    "    \"obs\": \"base\"\n",
    "}\n",
    "\n",
    "## define o prompt template\n",
    "prompt_template = templates.PROMPT_TEMPLATE_3\n",
    "config.update({\"prompt_template\": prompt_template})\n",
    "\n",
    "## define o prompt para formatar a resposta final \n",
    "#prompt_format = templates.PROMPT_TEMPLATE_ESTRUCTURE\n",
    "prompt_format = \"\" # para não utilizar \n",
    "config.update({\"prompt_format\": prompt_format})\n",
    "\n",
    "# load movie lens 100k dataset\n",
    "dataset = utils.read_json(f\"Data/{config['dataset']}.json\")\n",
    "print(f'Quantidade de Usuários: {len(dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.update(models_config.LlamaModels.llama_3_2_3b_instruct_Q4_K_M())\n",
    "config.update({\"obs\" : \"WM config\"})\n",
    "config.update({\"lenlimit_option\" : \"aleatorio\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models_config.LlamaModels.llama_3_2_3b_instruct_f16())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execução de configuração unitária"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifica configuração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executa 1 vez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pkl = recommender.recommendation_workflow_new(config         = config,\n",
    "                                                 dataset        = dataset,\n",
    "                                                 prompt_template= prompt_template,\n",
    "                                                 prompt_format  = prompt_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 vezes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "execucoes = 5\n",
    "for execucao in tqdm(range(execucoes),desc=f\"Execuções\"):\n",
    "    print(f'Execução {execucao+1} de {execucoes}')\n",
    "    result_pkl = recommender.recommendation_workflow_new(\n",
    "        config=config,\n",
    "        dataset=dataset,\n",
    "        prompt_template=prompt_template,\n",
    "        prompt_format=prompt_format\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execução multipla "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cria a lista de execuções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list = []\n",
    "\n",
    "# falta rodar 1 vez\n",
    "config1 = config.copy()\n",
    "config1.update(models_config.phi_4())\n",
    "config1.update({\"obs\" : \"base\"})\n",
    "config_list.append(config1)\n",
    "\n",
    "\n",
    "# esse llama 2 não está rodando\n",
    "#config3 = config.copy()\n",
    "#config3.update(models_config.LlamaModels.llama_2_7b_Q4_K_M())\n",
    "#config3.update({\"obs\" : \"base\"})\n",
    "#config_list.append(config3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### verifica lista de configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LLM_runtime': 'ROCm llama.cpp v1.29.0', 'dataset': 'ml_100k', 'nsu': 12, 'nci': 19, 'lenlimit': 8, 'lenlimit_option': 'aleatorio', 'test_run': 0, 'obs': 'base', 'prompt_template': {'System_prompt': \"You are a movie expert provide the answer for the question based on the given context. If you don't know the answer to a question, please don't share false information.\", 'Preference': '\\n    ### MY RECENTLY WATCHED MOVIES (FROM OLDEST TO NEWEST): {}.\\n\\n    ### QUESTION: Based on my watched movies list. Tell me what features are most important to me when selecting movies (Summarize my preferences briefly)?\\n\\n    ### ANSWER:\\n    ', 'Featured_movies': '\\n\\n    ### MY RECENTLY WATCHED MOVIES (FROM OLDEST TO NEWEST): {}.\\n\\n    ### MY MOVIE PREFERENCES: {}.\\n\\n    ### QUESTION: Create an enumerated list selecting the five most featured movies from the watched movies according to my movie preferences.\\n\\n    ### ANSWER:\\n    ', 'Recommendation': '\\n\\n    ### CANDIDATE MOVIE SET: {}.\\n\\n    ### MY RECENTLY WATCHED MOVIES (FROM OLDEST TO NEWEST): {}.\\n\\n    ### MY MOVIE PREFERENCES: {}.\\n\\n    ### MY FIVE MOST FEATURED MOVIES: {}.\\n\\n    ### INSTRUCTIONS:\\n    Recommend exactly **10 movies** from the \"Candidate Movie Set\" that are most similar to my \"Five Most Featured Movies\".  \\n    The output must be **STRICTLY formatted** as follows:\\n\\n    1. Movie Title  \\n    2. Movie Title  \\n    3. Movie Title  \\n    4. Movie Title  \\n    5. Movie Title  \\n    6. Movie Title  \\n    7. Movie Title  \\n    8. Movie Title  \\n    9. Movie Title  \\n    10. Movie Title  \\n\\n    Do not include any extra text before or after the list \\n    \\n    ### QUESTION:  \\n    What are your top 10 movie recommendations?  \\n\\n    ### ANSWER:\\n'}, 'prompt_format': '', 'model_name': 'gemma-2-9b-it', 'Arch': 'gemma2', 'Quantization': 'Q4_K_M', 'Temperature': 0.0, 'max_tokens': 4096, 'GPU Offload': 37, 'CPU Thread Pool Size': 6, 'Evaluation Batch Size': 512, 'Flash Attention': False}\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(config_list)):\n",
    "    config = config_list[i]\n",
    "    print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executa lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'prompt template: {prompt_template}')\n",
    "print(f'prompt format: {prompt_format}')\n",
    "\n",
    "### quantidade de keys no prompttemplate\n",
    "print(f'Quantidade de keys no prompt template: {len(prompt_template.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d013d42dee6c4f7f896c3625da78f5bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Configurações:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97de0f8b1f754ef79b04ded8f46d4049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Execuções da configuração 1:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execução 1 de 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "158879cc4a1a4079b989f4dca48a0701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processando:   0%|          | 0/943 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execução 1 de 3 finalizada\n",
      "Execução 2 de 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae30f588ec0243dc8e50b73ad1df1287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processando:   0%|          | 0/943 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execução 2 de 3 finalizada\n",
      "Execução 3 de 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "998bd5824ce043aca79bf2d74caf1e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processando:   0%|          | 0/943 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mExecução \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexecucao+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m de \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexecucoes\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     result_pkl = \u001b[43mrecommender\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecommendation_workflow_new\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m         \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m                                                    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m        \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m                                                    \u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m                                                    \u001b[49m\u001b[43mprompt_format\u001b[49m\u001b[43m  \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mExecução \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexecucao+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m de \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexecucoes\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m finalizada\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Comparing-LLM-Recs/Utils/recommender.py:219\u001b[39m, in \u001b[36mrecommendation_workflow_new\u001b[39m\u001b[34m(config, dataset, prompt_template, prompt_format)\u001b[39m\n\u001b[32m    217\u001b[39m input_1 = prompt_template[\u001b[33m'\u001b[39m\u001b[33mPreference\u001b[39m\u001b[33m'\u001b[39m].format(\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(watched_mv))\n\u001b[32m    218\u001b[39m results[i][\u001b[33m'\u001b[39m\u001b[33minput_1\u001b[39m\u001b[33m'\u001b[39m] = input_1\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m response = \u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery_lm_studio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mprompt_template\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mSystem_prompt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43minput_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m predictions_1 = response\n\u001b[32m    221\u001b[39m results[i][\u001b[33m'\u001b[39m\u001b[33mpredictions_1\u001b[39m\u001b[33m'\u001b[39m] = predictions_1\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Comparing-LLM-Recs/Utils/utils.py:39\u001b[39m, in \u001b[36mquery_lm_studio\u001b[39m\u001b[34m(model, temp, sytem_prompt, prompt, max_tokens)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(retries):\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m         response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAPI_URL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m response.status_code == \u001b[32m200\u001b[39m:\n\u001b[32m     42\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m response.json()[\u001b[33m\"\u001b[39m\u001b[33mchoices\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m].strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Comparing-LLM-Recs/.venv/lib/python3.12/site-packages/requests/api.py:115\u001b[39m, in \u001b[36mpost\u001b[39m\u001b[34m(url, data, json, **kwargs)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(url, data=\u001b[38;5;28;01mNone\u001b[39;00m, json=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    104\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    112\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Comparing-LLM-Recs/.venv/lib/python3.12/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Comparing-LLM-Recs/.venv/lib/python3.12/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Comparing-LLM-Recs/.venv/lib/python3.12/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Comparing-LLM-Recs/.venv/lib/python3.12/site-packages/requests/adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    664\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    682\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Comparing-LLM-Recs/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Comparing-LLM-Recs/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    532\u001b[39m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    536\u001b[39m     \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Comparing-LLM-Recs/.venv/lib/python3.12/site-packages/urllib3/connection.py:516\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    513\u001b[39m _shutdown = \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.sock, \u001b[33m\"\u001b[39m\u001b[33mshutdown\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    515\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m516\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    519\u001b[39m     assert_header_parsing(httplib_response.msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:1428\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1426\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1427\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1428\u001b[39m         \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1429\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[32m   1430\u001b[39m         \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:331\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m331\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n\u001b[32m    333\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/http/client.py:292\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    294\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mstatus line\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/socket.py:707\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m707\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    708\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    709\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "execucoes = 3\n",
    "\n",
    "for i in tqdm(range(0, len(config_list)),desc=\"Configurações\"):\n",
    "    config = config_list[i]\n",
    "    dataset = utils.read_json(f\"Data/{config['dataset']}.json\")\n",
    "    #print(f'Rodando configuração {i+1} de {len(config_list)}')\n",
    "\n",
    "    for execucao in tqdm(range(execucoes),desc=f\"Execuções da configuração {i+1}\"):\n",
    "        print(f'Execução {execucao+1} de {execucoes}')\n",
    "        try:\n",
    "            result_pkl = recommender.recommendation_workflow_new(config         = config,\n",
    "                                                            dataset        = dataset,\n",
    "                                                            prompt_template= prompt_template,\n",
    "                                                            prompt_format  = prompt_format)\n",
    "            print(f'Execução {execucao+1} de {execucoes} finalizada')\n",
    "        except Exception as e:\n",
    "            print(f'Erro na Execução {execucao+1} da configuração {i+1}')\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "    print(f'Configuração {i+1} de {len(config_list)} finalizada')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# outros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retorna apenas o usuários que tem o gt no candidatos.\n",
    "# implementar sobre o pipeline do projeto \n",
    "\n",
    "def get_candidate_ids_list(data, id_list, user_matrix_sim, num_u, num_i):\n",
    "    cand_ids = []\n",
    "    for i in id_list:\n",
    "        watched_movies = data[i][0].split(' | ')\n",
    "        candidate_items = utils.sort_user_filtering_items(data, watched_movies, user_matrix_sim[i], num_u, num_i)\n",
    "        if data[i][-1] in candidate_items:\n",
    "            cand_ids.append(i)\n",
    "    return cand_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = list(range(0, len(dataset)))\n",
    "#assert(len(id_list) == 943) # aqui é verificado se a lista possue exatamente essa quantidade\n",
    "\n",
    "# Building indexes and similarity matrices for users and movies.\n",
    "movie_idx = utils.build_moviename_index_dict(dataset)\n",
    "user_sim_matrix = utils.build_user_similarity_matrix(dataset, movie_idx)\n",
    "\n",
    "\n",
    "# para fazer a filtragem sobre os filmes \n",
    "#pop_dict = utils.build_movie_popularity_dict(dataset) \n",
    "#item_sim_matrix = utils.build_item_similarity_matrix(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pkl = recommender.recommendation_workflow(config         = config,\n",
    "                                                 dataset        = dataset,\n",
    "                                                 prompt_template= prompt_template,\n",
    "                                                 prompt_format  = prompt_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## execução varias configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from Utils import templates, utils, recommender\n",
    "\n",
    "## PROMPT 2 \n",
    "\n",
    "config = {}\n",
    "\n",
    "## define o prompt template\n",
    "prompt_template = templates.PROMPT_TEMPLATE_2\n",
    "config.update({\"prompt_template\": prompt_template})\n",
    "\n",
    "## define o prompt para formatar a resposta final \n",
    "#prompt_format = templates.PROMPT_TEMPLATE_ESTRUCTURE\n",
    "prompt_format = \"\" # para não utilizar \n",
    "config.update({\"prompt_format\": prompt_format})\n",
    "\n",
    "# load movie lens 100k dataset\n",
    "dataset = utils.read_json(\"Data/ML100K_clean.json\")\n",
    "print(f'Quantidade de Usuários: {len(dataset)}')\n",
    "\n",
    "config.update({\n",
    "    #\"runtime\": \"ROCm llama.cpp v1.23.0\", \n",
    "    #\"runtime\": \"CPU llama.cpp v1.22.2\", # performance ruim\n",
    "    \"LLM_runtime\": \"Vulkan llama.cpp v1.23.0\", # melhor opção\n",
    "    \"dataset\": \"ml_100k\",\n",
    "    \"nsu\" : 19,     # número de usuários para filtragem colaborativa        | SSBD :12  | Default :18   | \n",
    "    \"nci\" :19,      # número de itens para filtragem colaborativa           | SSBD :19  | Default :24   | Max : 1682\n",
    "    \"lenlimit\" : 8,  # limite de tamanho para a lista filmes assistidos    | SSBD : 8  | Default :24   | Max : 1682\n",
    "    \"test_run\" : 0, # define a quantidade de recomendações,                 |           | Default :0    | Max : 943 \n",
    "    \"obs\": \"testando prompt novo - prompt2\"\n",
    "})\n",
    "\n",
    "config.update({\n",
    "    \"model_name\" :\"gemma-3-4b-it\",\n",
    "    \"Arch\" : \"gemma3\",\n",
    "    \"Quantization\" : \"Q4_K_M\",\n",
    "    \"Temperature\": 0.1,\n",
    "    \"max_tokens\" : -1,  # Default : 4096\n",
    "    \"GPU Offload\": 34,\n",
    "    \"CPU Thread Pool Size\": 6,\n",
    "    \"Evaluation Batch Size\": 512,\n",
    "    \"Flash Attention\": False, # não vi vantagem no uso \n",
    "})\n",
    "\n",
    "config_list = []\n",
    "\n",
    "config1 = config.copy()\n",
    "config_list.append(config1)\n",
    "\n",
    "for i in range(0, len(config_list)):\n",
    "    config = config_list[i]\n",
    "    print(f'Rodando configuração {i} de {len(config_list)}')\n",
    "    try:\n",
    "        result_pkl = recommender.recommendation_workflow(config         = config,\n",
    "                                                         dataset        = dataset,\n",
    "                                                        prompt_template= prompt_template,\n",
    "                                                        prompt_format  = prompt_format)\n",
    "        print(f'Configuração {i} de {len(config_list)} finalizada')\n",
    "    except Exception as e:\n",
    "        print(f'Erro na configuração {i} de {len(config_list)}')\n",
    "        print(e)\n",
    "        continue\n",
    "\n",
    "## PROMPT 3 \n",
    "\n",
    "config = {}\n",
    "\n",
    "## define o prompt template\n",
    "prompt_template = templates.PROMPT_TEMPLATE_3\n",
    "config.update({\"prompt_template\": prompt_template})\n",
    "\n",
    "## define o prompt para formatar a resposta final \n",
    "#prompt_format = templates.PROMPT_TEMPLATE_ESTRUCTURE\n",
    "prompt_format = \"\" # para não utilizar \n",
    "config.update({\"prompt_format\": prompt_format})\n",
    "\n",
    "# load movie lens 100k dataset\n",
    "dataset = utils.read_json(\"Data/ML100K_clean.json\")\n",
    "print(f'Quantidade de Usuários: {len(dataset)}')\n",
    "\n",
    "config.update({\n",
    "    #\"runtime\": \"ROCm llama.cpp v1.23.0\", \n",
    "    #\"runtime\": \"CPU llama.cpp v1.22.2\", # performance ruim\n",
    "    \"LLM_runtime\": \"Vulkan llama.cpp v1.23.0\", # melhor opção\n",
    "    \"dataset\": \"ml_100k\",\n",
    "    \"nsu\" : 12,     # número de usuários para filtragem colaborativa        | SSBD :12  | Default :18   | \n",
    "    \"nci\" :19,      # número de itens para filtragem colaborativa           | SSBD :19  | Default :24   | Max : 1682\n",
    "    \"lenlimit\" : 8,  # limite de tamanho para a lista filmes assistidos    | SSBD : 8  | Default :24   | Max : 1682\n",
    "    \"test_run\" : 0, # define a quantidade de recomendações,                 |           | Default :0    | Max : 943 \n",
    "    \"obs\": \"testando prompt novo - prompt3\"\n",
    "})\n",
    "\n",
    "config.update({\n",
    "    \"model_name\" :\"gemma-3-4b-it\",\n",
    "    \"Arch\" : \"gemma3\",\n",
    "    \"Quantization\" : \"Q4_K_M\",\n",
    "    \"Temperature\": 0.1,\n",
    "    \"max_tokens\" : -1,  # Default : 4096\n",
    "    \"GPU Offload\": 34,\n",
    "    \"CPU Thread Pool Size\": 6,\n",
    "    \"Evaluation Batch Size\": 512,\n",
    "    \"Flash Attention\": False, # não vi vantagem no uso \n",
    "})\n",
    "\n",
    "config_list = []\n",
    "\n",
    "config1 = config.copy()\n",
    "config_list.append(config1)\n",
    "\n",
    "for i in range(0, len(config_list)):\n",
    "    config = config_list[i]\n",
    "    print(f'Rodando configuração {i} de {len(config_list)}')\n",
    "    try:\n",
    "        result_pkl = recommender.recommendation_workflow(config         = config,\n",
    "                                                         dataset        = dataset,\n",
    "                                                        prompt_template= prompt_template,\n",
    "                                                        prompt_format  = prompt_format)\n",
    "        print(f'Configuração {i} de {len(config_list)} finalizada')\n",
    "    except Exception as e:\n",
    "        print(f'Erro na configuração {i} de {len(config_list)}')\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(config_list)):\n",
    "    config = config_list[i]\n",
    "    print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{result_pkl}', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for key, value in data.items():\n",
    "    if isinstance(key, int) and isinstance(value, dict):  # Pegando apenas os experimentos\n",
    "        results.append({\n",
    "            'Candidates': value.get('candidate_set', ''),\n",
    "            'Ground Truth': value.get('ground_truth', ''),\n",
    "            'gt_in_candidate_set': value.get('gt_in_candidate_set', ''),\n",
    "            #'Input 1': value.get('input_1', ''),\n",
    "            #'Predictions 1': value.get('predictions_1', ''),\n",
    "            #'Input 2': value.get('input_2', ''),\n",
    "            #'Predictions 2': value.get('predictions_2', ''),\n",
    "            'Input 3': value.get('input_3', ''),\n",
    "            'Predictions 3': value.get('predictions_3', ''),\n",
    "            #'Recommendations': value.get('recommendations', ''),\n",
    "            'rec_HitRate@10': value.get('rec_HitRate@10', ''),\n",
    "            #'Precision': value.get('precision', ''),\n",
    "            #'Recall': value.get('recall', ''),\n",
    "            'rec_NDCG@10': value.get('rec_NDCG@10', '')\n",
    "        })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparar dataset / dividir em treino e teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cria uma lista dos usuários com GT na lista de candidatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from Utils import templates, utils\n",
    "\n",
    "nsu = 12\n",
    "nci = 19\n",
    "lenlimit = 8 \n",
    "\n",
    "dataset = utils.read_json(\"Data/ML100K_clean.json\")\n",
    "movie_idx = utils.build_moviename_index_dict(dataset)\n",
    "user_sim_matrix = utils.build_user_similarity_matrix(dataset, movie_idx)\n",
    "id_list = list(range(0, len(dataset)))\n",
    "\n",
    "data_list = []\n",
    "id_list_com_gt_no_candidate=[]\n",
    "\n",
    "for i in id_list:\n",
    "\n",
    "  watched_mv = dataset[i][0].split(' | ')[::-1]\n",
    "  watched_mv = watched_mv[-lenlimit:]\n",
    "  \n",
    "  groundTruth = dataset[i][-1]\n",
    "  candidate_items = utils.sort_collaborative_user_filtering(target_user_id=i,\n",
    "                                                                    dataset=dataset,\n",
    "                                                                    user_similarity_matrix=user_sim_matrix,\n",
    "                                                                    num_users=nsu,\n",
    "                                                                    num_items=nci,\n",
    "                                                                    include_similar_user_GT=False,\n",
    "                                                                    debug=False)\n",
    "  random.shuffle(candidate_items)\n",
    "\n",
    "  # verifica se o ground_truth está no candidate_set\n",
    "  gt_in_candidate_set = True if any(groundTruth.lower() in candidate.lower() for candidate in candidate_items) else False\n",
    "\n",
    "  if gt_in_candidate_set == True:\n",
    "    id_list_com_gt_no_candidate.append(i)\n",
    "\n",
    "  data_list.append({\n",
    "        'user_id': i,\n",
    "        'watched_movies': watched_mv,\n",
    "        'ground_truth': groundTruth,\n",
    "        'candidate_items': candidate_items,\n",
    "        'gt_in_candidate_set': gt_in_candidate_set\n",
    "  })\n",
    "\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "print(f'nsu: {nsu} \\nnci: {nci} \\nQnt de usuários do dataset: {len(dataset)} \\nQnt de usuários com gt no candidate: {len(id_list_com_gt_no_candidate)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realiza a divisão de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a proporção de divisão\n",
    "train_ratio = 0.8\n",
    "test_ratio = 0.2\n",
    "\n",
    "# Embaralha os IDs para garantir aleatoriedade\n",
    "random.shuffle(id_list_com_gt_no_candidate)\n",
    "\n",
    "# Calcula o tamanho de cada conjunto\n",
    "total_ids = len(id_list_com_gt_no_candidate)\n",
    "train_size = int(train_ratio * total_ids)\n",
    "test_size = total_ids - train_size\n",
    "\n",
    "# Separa os IDs em conjuntos\n",
    "train_ids = id_list_com_gt_no_candidate[:train_size]\n",
    "test_ids = id_list_com_gt_no_candidate[train_size:]\n",
    "\n",
    "# Imprime o tamanho de cada conjunto para verificação\n",
    "print(f\"Tamanho do conjunto de treino: {len(train_ids)}\")\n",
    "print(f\"Tamanho do conjunto de teste: {len(test_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajusta o dataset de treino, adicionando as instruções necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = []\n",
    "\n",
    "# configs \n",
    "model_name = \"gemma-3-4b-it\"\n",
    "temperature = 0\n",
    "prompt_template = templates.PROMPT_TEMPLATE_3\n",
    "system_prompt = \"You are a movie expert provide the answer for the question based on the given context. Your answer must be short \" \n",
    "max_tokens = -1\n",
    "lenlimit = 8\n",
    "\n",
    "for id in train_ids:\n",
    "    filtered = df[df['user_id'] == id]\n",
    "    if not filtered.empty:\n",
    "        candidate_items = filtered['candidate_items'].iloc[0]\n",
    "        ground_truth = filtered['ground_truth'].iloc[0]\n",
    "        watched_movies = filtered['watched_movies'].iloc[0]\n",
    "    else:\n",
    "        continue \n",
    "\n",
    "    input_prompt = prompt_template['Preference'].format(', '.join(candidate_items),', '.join(watched_movies))\n",
    "    response = utils.query_lm_studio(model_name,0.0,system_prompt,input_prompt,max_tokens)\n",
    "    movie_preference = response\n",
    "\n",
    "    input_prompt = prompt_template['Featured_movies'].format(', '.join(watched_mv), movie_preference)\n",
    "    response = utils.query_lm_studio(model_name,0.0,system_prompt,input_prompt,max_tokens)\n",
    "    most_featured = response\n",
    "\n",
    "    input = templates.TRAIN_TEMPLATE_2['INPUT'].format(', '.join(candidate_items),', '.join(watched_movies),movie_preference,most_featured )\n",
    "    output= templates.TRAIN_TEMPLATE_2['OUTPUT'].format(ground_truth)\n",
    "\n",
    "    train_dataset.append({\n",
    "        'input': input,\n",
    "        'output': output\n",
    "  })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(train_dataset)\n",
    "df_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salva arquivo de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_pickle('Data/ML100K_train.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salva arquivo de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = []\n",
    "\n",
    "for id in test_ids:\n",
    "    filtered = df[df['user_id'] == id]\n",
    "    if not filtered.empty:\n",
    "        candidate_items = filtered['candidate_items'].iloc[0]\n",
    "        ground_truth = filtered['ground_truth'].iloc[0]\n",
    "        watched_movies = filtered['watched_movies'].iloc[0]\n",
    "    else:\n",
    "        continue \n",
    "    \n",
    "    test_dataset.append({\n",
    "        'user_id': id,\n",
    "        'candidate_items': candidate_items,\n",
    "        'ground_truth': ground_truth,\n",
    "        'watched_movies': watched_movies\n",
    "    })\n",
    "df_test = pd.DataFrame(test_dataset)\n",
    "df_test.to_pickle('Data/ML100K_test.pkl')\n",
    "df_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
