{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuração inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from Utils import templates, utils, recommender, models_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de Usuários: 943\n"
     ]
    }
   ],
   "source": [
    "## Configurações base\n",
    "\n",
    "config = {\n",
    "    \"LLM_runtime\": \"ROCm llama.cpp v1.31.0\", # melhor opção no momento (ultimo teste: 03/05/2025)\n",
    "    #\"LLM_runtime\": \"CPU llama.cpp v1.22.2\", # performance ruim\n",
    "    #\"LLM_runtime\": \"Vulkan llama.cpp v1.28.0\", # performance ruim\n",
    "    \"dataset\": \"ml_100k\",                                                  #| Opções 'ml_100k' e 'ml_1m'\n",
    "    \"nsu\" : 12,     # número de usuários para filtragem colaborativa        | SSBD :12  | Default :18   |            | Best = 19\n",
    "    \"nci\" :19,      # número de itens para filtragem colaborativa           | SSBD :19  | Default :24   | Max : 1682 |\n",
    "    \"lenlimit\" : 8,  # limite de tamanho para a lista filmes assistidos     | SSBD : 8  | Default :24   | Max : 1682 | Best = 8\n",
    "    \"lenlimit_option\" : 'aleatorio', # define qual a abordagem                | opções : 'ultimos', 'primeiros', 'aleatorio' | Default : 'aleatorio'\n",
    "    \"test_run\" : 0, # define a quantidade de recomendações,                 |           | Default :0    | Max : 943 \n",
    "    \"obs\": \"base\"\n",
    "}\n",
    "\n",
    "## define o prompt template\n",
    "prompt_template = templates.PROMPT_TEMPLATE_3\n",
    "config.update({\"prompt_template\": prompt_template})\n",
    "\n",
    "## define o prompt para formatar a resposta final \n",
    "#prompt_format = templates.PROMPT_TEMPLATE_ESTRUCTURE\n",
    "prompt_format = \"\" # para não utilizar \n",
    "config.update({\"prompt_format\": prompt_format})\n",
    "\n",
    "# load movie lens 100k dataset\n",
    "dataset = utils.read_json(f\"Data/{config['dataset']}.json\")\n",
    "print(f'Quantidade de Usuários: {len(dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.update(models_config.LlamaModels.llama_3_2_3b_instruct_Q4_K_M())\n",
    "config.update({\"obs\" : \"WM config\"})\n",
    "config.update({\"lenlimit_option\" : \"aleatorio\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models_config.LlamaModels.llama_3_2_3b_instruct_f16())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execução de configuração unitária"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifica configuração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executa 1 vez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pkl = recommender.recommendation_workflow_new(config         = config,\n",
    "                                                 dataset        = dataset,\n",
    "                                                 prompt_template= prompt_template,\n",
    "                                                 prompt_format  = prompt_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 vezes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "execucoes = 5\n",
    "for execucao in tqdm(range(execucoes),desc=f\"Execuções\"):\n",
    "    print(f'Execução {execucao+1} de {execucoes}')\n",
    "    result_pkl = recommender.recommendation_workflow_new(\n",
    "        config=config,\n",
    "        dataset=dataset,\n",
    "        prompt_template=prompt_template,\n",
    "        prompt_format=prompt_format\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execução multipla "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cria a lista de execuções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list = []\n",
    "\n",
    "config1 = config.copy()\n",
    "config1.update(models_config.LlamaModels.llama_3_2_3b_instruct_Q4_K_M())\n",
    "config1.update({\"dataset\" : \"ml_1m\"})\n",
    "config1.update({\"obs\" : \"ml_1m\"})\n",
    "config_list.append(config1)\n",
    "\n",
    "config2 = config.copy()\n",
    "config2.update(models_config.LlamaModels.llama_3_2_3b_instruct_Q4_K_M())\n",
    "config2.update({\"lenlimit\" : 1682})\n",
    "config2.update({\"obs\" : \"lenlimit 1682\"})\n",
    "config_list.append(config2)\n",
    "\n",
    "# esse llama 2 não está rodando\n",
    "#config3 = config.copy()\n",
    "#config3.update(models_config.LlamaModels.llama_2_7b_Q4_K_M())\n",
    "#config3.update({\"obs\" : \"base\"})\n",
    "#config_list.append(config3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### verifica lista de configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LLM_runtime': 'ROCm llama.cpp v1.31.0', 'dataset': 'ml_100k', 'nsu': 12, 'nci': 19, 'lenlimit': 24, 'lenlimit_option': 'aleatorio', 'test_run': 0, 'obs': 'lenlimit 24', 'prompt_template': {'System_prompt': \"You are a movie expert provide the answer for the question based on the given context. If you don't know the answer to a question, please don't share false information.\", 'Preference': '\\n    ### MY RECENTLY WATCHED MOVIES (FROM OLDEST TO NEWEST): {}.\\n\\n    ### QUESTION: Based on my watched movies list. Tell me what features are most important to me when selecting movies (Summarize my preferences briefly)?\\n\\n    ### ANSWER:\\n    ', 'Featured_movies': '\\n\\n    ### MY RECENTLY WATCHED MOVIES (FROM OLDEST TO NEWEST): {}.\\n\\n    ### MY MOVIE PREFERENCES: {}.\\n\\n    ### QUESTION: Create an enumerated list selecting the five most featured movies from the watched movies according to my movie preferences.\\n\\n    ### ANSWER:\\n    ', 'Recommendation': '\\n\\n    ### CANDIDATE MOVIE SET: {}.\\n\\n    ### MY RECENTLY WATCHED MOVIES (FROM OLDEST TO NEWEST): {}.\\n\\n    ### MY MOVIE PREFERENCES: {}.\\n\\n    ### MY FIVE MOST FEATURED MOVIES: {}.\\n\\n    ### INSTRUCTIONS:\\n    Recommend exactly **10 movies** from the \"Candidate Movie Set\" that are most similar to my \"Five Most Featured Movies\".  \\n    The output must be **STRICTLY formatted** as follows:\\n\\n    1. Movie Title  \\n    2. Movie Title  \\n    3. Movie Title  \\n    4. Movie Title  \\n    5. Movie Title  \\n    6. Movie Title  \\n    7. Movie Title  \\n    8. Movie Title  \\n    9. Movie Title  \\n    10. Movie Title  \\n\\n    Do not include any extra text before or after the list \\n    \\n    ### QUESTION:  \\n    What are your top 10 movie recommendations?  \\n\\n    ### ANSWER:\\n'}, 'prompt_format': '', 'model_name': 'meta-llama-3-8b-instruct', 'Arch': 'llama', 'Quantization': 'Q4_K_M', 'Temperature': 0.0, 'max_tokens': 4096, 'GPU Offload': 32, 'CPU Thread Pool Size': 6, 'Evaluation Batch Size': 512, 'Flash Attention': False}\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(config_list)):\n",
    "    config = config_list[i]\n",
    "    print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executa lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'prompt template: {prompt_template}')\n",
    "print(f'prompt format: {prompt_format}')\n",
    "\n",
    "### quantidade de keys no prompttemplate\n",
    "print(f'Quantidade de keys no prompt template: {len(prompt_template.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f118d25b84e4076b16b1ae2bd281b73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Configurações:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "803fcf4568c2473ea2b9e35ec318e196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Execuções da configuração 1:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execução 1 de 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e481a9a55c5b4d3282dd9663f8e458ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processando:   0%|          | 0/6040 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execução 1 de 5 finalizada\n",
      "Execução 2 de 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eeda9f3d57d4a7a8a090c872c1ef1cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processando:   0%|          | 0/6040 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execução 2 de 5 finalizada\n",
      "Execução 3 de 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74a83f7426a44d98a0040d5ce90b8cdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processando:   0%|          | 0/6040 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execução 3 de 5 finalizada\n",
      "Execução 4 de 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fe4c7aaced74b688aaded78233e7415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processando:   0%|          | 0/6040 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execução 4 de 5 finalizada\n",
      "Execução 5 de 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7755a38a4fb471594c16607b95668ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processando:   0%|          | 0/6040 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execução 5 de 5 finalizada\n",
      "Configuração 1 de 2 finalizada\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c81d06f48744680ae4584a99c744ed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Execuções da configuração 2:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execução 1 de 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae6bafccc5264c6ab8df38a28596b5a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processando:   0%|          | 0/943 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execução 1 de 5 finalizada\n",
      "Execução 2 de 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1176e9104e5482cb17a1b9ea1bd6389",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processando:   0%|          | 0/943 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execução 2 de 5 finalizada\n",
      "Execução 3 de 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "721f3f1fab494072a4ab7dda1ae30780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processando:   0%|          | 0/943 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execução 3 de 5 finalizada\n",
      "Execução 4 de 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f1c867628542749e805e33637004a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processando:   0%|          | 0/943 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execução 4 de 5 finalizada\n",
      "Execução 5 de 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b16b7f608d04bc3b887c3223672c5a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processando:   0%|          | 0/943 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execução 5 de 5 finalizada\n",
      "Configuração 2 de 2 finalizada\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "execucoes = 5\n",
    "\n",
    "for i in tqdm(range(0, len(config_list)),desc=\"Configurações\"):\n",
    "    config = config_list[i]\n",
    "    dataset = utils.read_json(f\"Data/{config['dataset']}.json\")\n",
    "    #print(f'Rodando configuração {i+1} de {len(config_list)}')\n",
    "\n",
    "    for execucao in tqdm(range(execucoes),desc=f\"Execuções da configuração {i+1}\"):\n",
    "        print(f'Execução {execucao+1} de {execucoes}')\n",
    "        try:\n",
    "            result_pkl = recommender.recommendation_workflow_new(config         = config,\n",
    "                                                            dataset        = dataset,\n",
    "                                                            prompt_template= prompt_template,\n",
    "                                                            prompt_format  = prompt_format)\n",
    "            print(f'Execução {execucao+1} de {execucoes} finalizada')\n",
    "        except Exception as e:\n",
    "            print(f'Erro na Execução {execucao+1} da configuração {i+1}')\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "    print(f'Configuração {i+1} de {len(config_list)} finalizada')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# outros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realação de usuários e usuários com GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users,gt_users = utils.total_users_and_gt_users(config)\n",
    "print(f'Quantidade de usuários: {users}')\n",
    "print(f'Quantidade de usuários com gt: {gt_users}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparar dataset / dividir em treino e teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cria uma lista dos usuários com GT na lista de candidatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from Utils import templates, utils\n",
    "\n",
    "nsu = 12\n",
    "nci = 19\n",
    "lenlimit = 8 \n",
    "\n",
    "dataset = utils.read_json(\"Data/ML100K_clean.json\")\n",
    "movie_idx = utils.build_moviename_index_dict(dataset)\n",
    "user_sim_matrix = utils.build_user_similarity_matrix(dataset, movie_idx)\n",
    "id_list = list(range(0, len(dataset)))\n",
    "\n",
    "data_list = []\n",
    "id_list_com_gt_no_candidate=[]\n",
    "\n",
    "for i in id_list:\n",
    "\n",
    "  watched_mv = dataset[i][0].split(' | ')[::-1]\n",
    "  watched_mv = watched_mv[-lenlimit:]\n",
    "  \n",
    "  groundTruth = dataset[i][-1]\n",
    "  candidate_items = utils.sort_collaborative_user_filtering(target_user_id=i,\n",
    "                                                                    dataset=dataset,\n",
    "                                                                    user_similarity_matrix=user_sim_matrix,\n",
    "                                                                    num_users=nsu,\n",
    "                                                                    num_items=nci,\n",
    "                                                                    include_similar_user_GT=False,\n",
    "                                                                    debug=False)\n",
    "  random.shuffle(candidate_items)\n",
    "\n",
    "  # verifica se o ground_truth está no candidate_set\n",
    "  gt_in_candidate_set = True if any(groundTruth.lower() in candidate.lower() for candidate in candidate_items) else False\n",
    "\n",
    "  if gt_in_candidate_set == True:\n",
    "    id_list_com_gt_no_candidate.append(i)\n",
    "\n",
    "  data_list.append({\n",
    "        'user_id': i,\n",
    "        'watched_movies': watched_mv,\n",
    "        'ground_truth': groundTruth,\n",
    "        'candidate_items': candidate_items,\n",
    "        'gt_in_candidate_set': gt_in_candidate_set\n",
    "  })\n",
    "\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "print(f'nsu: {nsu} \\nnci: {nci} \\nQnt de usuários do dataset: {len(dataset)} \\nQnt de usuários com gt no candidate: {len(id_list_com_gt_no_candidate)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realiza a divisão de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a proporção de divisão\n",
    "train_ratio = 0.8\n",
    "test_ratio = 0.2\n",
    "\n",
    "# Embaralha os IDs para garantir aleatoriedade\n",
    "random.shuffle(id_list_com_gt_no_candidate)\n",
    "\n",
    "# Calcula o tamanho de cada conjunto\n",
    "total_ids = len(id_list_com_gt_no_candidate)\n",
    "train_size = int(train_ratio * total_ids)\n",
    "test_size = total_ids - train_size\n",
    "\n",
    "# Separa os IDs em conjuntos\n",
    "train_ids = id_list_com_gt_no_candidate[:train_size]\n",
    "test_ids = id_list_com_gt_no_candidate[train_size:]\n",
    "\n",
    "# Imprime o tamanho de cada conjunto para verificação\n",
    "print(f\"Tamanho do conjunto de treino: {len(train_ids)}\")\n",
    "print(f\"Tamanho do conjunto de teste: {len(test_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajusta o dataset de treino, adicionando as instruções necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = []\n",
    "\n",
    "# configs \n",
    "model_name = \"gemma-3-4b-it\"\n",
    "temperature = 0\n",
    "prompt_template = templates.PROMPT_TEMPLATE_3\n",
    "system_prompt = \"You are a movie expert provide the answer for the question based on the given context. Your answer must be short \" \n",
    "max_tokens = -1\n",
    "lenlimit = 8\n",
    "\n",
    "for id in train_ids:\n",
    "    filtered = df[df['user_id'] == id]\n",
    "    if not filtered.empty:\n",
    "        candidate_items = filtered['candidate_items'].iloc[0]\n",
    "        ground_truth = filtered['ground_truth'].iloc[0]\n",
    "        watched_movies = filtered['watched_movies'].iloc[0]\n",
    "    else:\n",
    "        continue \n",
    "\n",
    "    input_prompt = prompt_template['Preference'].format(', '.join(candidate_items),', '.join(watched_movies))\n",
    "    response = utils.query_lm_studio(model_name,0.0,system_prompt,input_prompt,max_tokens)\n",
    "    movie_preference = response\n",
    "\n",
    "    input_prompt = prompt_template['Featured_movies'].format(', '.join(watched_mv), movie_preference)\n",
    "    response = utils.query_lm_studio(model_name,0.0,system_prompt,input_prompt,max_tokens)\n",
    "    most_featured = response\n",
    "\n",
    "    input = templates.TRAIN_TEMPLATE_2['INPUT'].format(', '.join(candidate_items),', '.join(watched_movies),movie_preference,most_featured )\n",
    "    output= templates.TRAIN_TEMPLATE_2['OUTPUT'].format(ground_truth)\n",
    "\n",
    "    train_dataset.append({\n",
    "        'input': input,\n",
    "        'output': output\n",
    "  })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(train_dataset)\n",
    "df_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salva arquivo de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_pickle('Data/ML100K_train.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salva arquivo de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = []\n",
    "\n",
    "for id in test_ids:\n",
    "    filtered = df[df['user_id'] == id]\n",
    "    if not filtered.empty:\n",
    "        candidate_items = filtered['candidate_items'].iloc[0]\n",
    "        ground_truth = filtered['ground_truth'].iloc[0]\n",
    "        watched_movies = filtered['watched_movies'].iloc[0]\n",
    "    else:\n",
    "        continue \n",
    "    \n",
    "    test_dataset.append({\n",
    "        'user_id': id,\n",
    "        'candidate_items': candidate_items,\n",
    "        'ground_truth': ground_truth,\n",
    "        'watched_movies': watched_movies\n",
    "    })\n",
    "df_test = pd.DataFrame(test_dataset)\n",
    "df_test.to_pickle('Data/ML100K_test.pkl')\n",
    "df_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
