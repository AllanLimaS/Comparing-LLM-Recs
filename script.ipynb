{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuração inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from Utils import templates, utils, recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de Usuários: 943\n"
     ]
    }
   ],
   "source": [
    "## Configurações base\n",
    "\n",
    "config = {\n",
    "    #\"runtime\": \"ROCm llama.cpp v1.23.0\", \n",
    "    #\"runtime\": \"CPU llama.cpp v1.22.2\", # performance ruim\n",
    "    \"LLM_runtime\": \"Vulkan llama.cpp v1.23.0\", # melhor opção\n",
    "    \"dataset\": \"ml_100k\",\n",
    "    \"nsu\" : 18,     # número de usuários para filtragem colaborativa        | SSBD :12 | Default :18 | \n",
    "    \"nci\" :1682,      # número de itens para filtragem colaborativa           | SSBD :19 | Default :24 | Max : 1682\n",
    "    \"lenlimit\" : 1682,  # limite de tamanho para a lista filmes assistidos    | SSBD : 8 | Default :24 | Max : 1682\n",
    "    \"test_run\" : 20, # define a quantidade de recomendações, '0' para todos\n",
    "    \"obs\": \"Todos os filmes na lista de candidatos contexto 20028\"\n",
    "}\n",
    "\n",
    "## define o prompt template\n",
    "prompt_template = templates.PROMPT_TEMPLATE_2\n",
    "config.update({\"prompt_template\": prompt_template})\n",
    "\n",
    "## define o prompt para formatar a resposta final \n",
    "#prompt_format = templates.PROMPT_TEMPLATE_ESTRUCTURE\n",
    "prompt_format = \"\" # para não utilizar \n",
    "config.update({\"prompt_format\": prompt_format})\n",
    "\n",
    "# load movie lens 100k dataset\n",
    "dataset = utils.read_json(\"Data/ML100K_clean.json\")\n",
    "print(f'Quantidade de Usuários: {len(dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------   \n",
    "## Escolha do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gemma-3-4b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.update({\n",
    "    \"model_name\" :\"gemma-3-4b-it\",\n",
    "    \"Arch\" : \"gemma3\",\n",
    "    \"Quantization\" : \"Q4_K_M\",\n",
    "    \"Temperature\": 0.1,\n",
    "    \"max_tokens\" : -1,  # Default : 4096\n",
    "    \"GPU Offload\": 34,\n",
    "    \"CPU Thread Pool Size\": 6,\n",
    "    \"Evaluation Batch Size\": 512,\n",
    "    \"Flash Attention\": False, # não vi vantagem no uso \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### meta-llama-3.1-8b-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.update({\n",
    "    \"model_name\" :\"meta-llama-3.1-8b-instruct\",\n",
    "    \"Arch\" : \"llama\",\n",
    "    \"Quantization\" : \"Q4_K_M\",\n",
    "    \"Temperature\": 0.1,\n",
    "    \"max_tokens\" : 4096,\n",
    "    \"GPU Offload\": 34,\n",
    "    \"CPU Thread Pool Size\": 6,\n",
    "    \"Evaluation Batch Size\": 512,\n",
    "    \"Flash Attention\": False,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### llama-3.2-3b-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.update({\n",
    "    \"model_name\" :\"llama-3.2-3b-instruct\",\n",
    "    \"Arch\" : \"llama\",\n",
    "    \"Quantization\" : \"Q8_0\",\n",
    "    \"Temperature\": 0,\n",
    "    \"max_tokens\" : -1, #Default :4096\n",
    "    \"GPU Offload\": 34,\n",
    "    \"CPU Thread Pool Size\": 6,\n",
    "    \"Evaluation Batch Size\": 512,\n",
    "    \"Flash Attention\": False,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retorna apenas o usuários que tem o gt no candidatos.\n",
    "# implementar sobre o pipeline do projeto \n",
    "\n",
    "def get_candidate_ids_list(data, id_list, user_matrix_sim, num_u, num_i):\n",
    "    cand_ids = []\n",
    "    for i in id_list:\n",
    "        watched_movies = data[i][0].split(' | ')\n",
    "        candidate_items = utils.sort_user_filtering_items(data, watched_movies, user_matrix_sim[i], num_u, num_i)\n",
    "        if data[i][-1] in candidate_items:\n",
    "            cand_ids.append(i)\n",
    "    return cand_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = list(range(0, len(dataset)))\n",
    "#assert(len(id_list) == 943) # aqui é verificado se a lista possue exatamente essa quantidade\n",
    "\n",
    "# Building indexes and similarity matrices for users and movies.\n",
    "movie_idx = utils.build_moviename_index_dict(dataset)\n",
    "user_sim_matrix = utils.build_user_similarity_matrix(dataset, movie_idx)\n",
    "\n",
    "\n",
    "# para fazer a filtragem sobre os filmes \n",
    "#pop_dict = utils.build_movie_popularity_dict(dataset) \n",
    "#item_sim_matrix = utils.build_item_similarity_matrix(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3755dc96b3004e8caa3301d62a5c6bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processando:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_pkl = recommender.recommendation_workflow(config         = config,\n",
    "                                                 dataset        = dataset,\n",
    "                                                 prompt_template= prompt_template,\n",
    "                                                 prompt_format  = prompt_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{result_pkl}', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for key, value in data.items():\n",
    "    if isinstance(key, int) and isinstance(value, dict):  # Pegando apenas os experimentos\n",
    "        results.append({\n",
    "            'Candidates': value.get('candidate_set', ''),\n",
    "            'Ground Truth': value.get('ground_truth', ''),\n",
    "            'gt_in_candidate_set': value.get('gt_in_candidate_set', ''),\n",
    "            #'Input 1': value.get('input_1', ''),\n",
    "            #'Predictions 1': value.get('predictions_1', ''),\n",
    "            #'Input 2': value.get('input_2', ''),\n",
    "            #'Predictions 2': value.get('predictions_2', ''),\n",
    "            'Input 3': value.get('input_3', ''),\n",
    "            'Predictions 3': value.get('predictions_3', ''),\n",
    "            #'Recommendations': value.get('recommendations', ''),\n",
    "            'rec_HitRate@10': value.get('rec_HitRate@10', ''),\n",
    "            #'Precision': value.get('precision', ''),\n",
    "            #'Recall': value.get('recall', ''),\n",
    "            'rec_NDCG@10': value.get('rec_NDCG@10', '')\n",
    "        })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "df_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
