{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuração inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from Utils import templates, utils, recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de Usuários: 943\n"
     ]
    }
   ],
   "source": [
    "## Configurações base\n",
    "\n",
    "config = {\n",
    "    #\"runtime\": \"ROCm llama.cpp v1.23.0\", \n",
    "    #\"runtime\": \"CPU llama.cpp v1.22.2\", # performance ruim\n",
    "    \"LLM_runtime\": \"Vulkan llama.cpp v1.23.0\", # melhor opção\n",
    "    \"dataset\": \"ml_100k\",\n",
    "    \"nsu\" : 18,     # número de usuários para filtragem colaborativa        | SSBD :12  | Default :18   | \n",
    "    \"nci\" :24,      # número de itens para filtragem colaborativa           | SSBD :19  | Default :24   | Max : 1682\n",
    "    \"lenlimit\" : 24,  # limite de tamanho para a lista filmes assistidos    | SSBD : 8  | Default :24   | Max : 1682\n",
    "    \"test_run\" : 0, # define a quantidade de recomendações,                 |           | Default :0    | Max : 943 \n",
    "    \"obs\": \"base\"\n",
    "}\n",
    "\n",
    "## define o prompt template\n",
    "prompt_template = templates.PROMPT_TEMPLATE_2\n",
    "config.update({\"prompt_template\": prompt_template})\n",
    "\n",
    "## define o prompt para formatar a resposta final \n",
    "#prompt_format = templates.PROMPT_TEMPLATE_ESTRUCTURE\n",
    "prompt_format = \"\" # para não utilizar \n",
    "config.update({\"prompt_format\": prompt_format})\n",
    "\n",
    "# load movie lens 100k dataset\n",
    "dataset = utils.read_json(\"Data/ML100K_clean.json\")\n",
    "print(f'Quantidade de Usuários: {len(dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------   \n",
    "## Escolha do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mistral-7b-instruct-v0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.update({\n",
    "    \"model_name\" :\"mistral-7b-instruct-v0.3\",\n",
    "    \"Arch\" : \"llama\",\n",
    "    \"Quantization\" : \"Q4_K_M\",\n",
    "    \"Temperature\": 0.1,\n",
    "    \"max_tokens\" : -1,  # Default : 4096\n",
    "    \"GPU Offload\": 34,\n",
    "    \"CPU Thread Pool Size\": 6,\n",
    "    \"Evaluation Batch Size\": 512,\n",
    "    \"Flash Attention\": False, # não vi vantagem no uso \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gemma-3-4b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.update({\n",
    "    \"model_name\" :\"gemma-3-4b-it\",\n",
    "    \"Arch\" : \"gemma3\",\n",
    "    \"Quantization\" : \"Q4_K_M\",\n",
    "    \"Temperature\": 0.1,\n",
    "    \"max_tokens\" : -1,  # Default : 4096\n",
    "    \"GPU Offload\": 34,\n",
    "    \"CPU Thread Pool Size\": 6,\n",
    "    \"Evaluation Batch Size\": 512,\n",
    "    \"Flash Attention\": False, # não vi vantagem no uso \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### meta-llama-3.1-8b-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.update({\n",
    "    \"model_name\" :\"meta-llama-3.1-8b-instruct\",\n",
    "    \"Arch\" : \"llama\",\n",
    "    \"Quantization\" : \"Q4_K_M\",\n",
    "    \"Temperature\": 0.1,\n",
    "    \"max_tokens\" : 4096,\n",
    "    \"GPU Offload\": 34,\n",
    "    \"CPU Thread Pool Size\": 6,\n",
    "    \"Evaluation Batch Size\": 512,\n",
    "    \"Flash Attention\": False,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### llama-3.2-3b-instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.update({\n",
    "    \"model_name\" :\"llama-3.2-3b-instruct\",\n",
    "    \"Arch\" : \"llama\",\n",
    "    \"Quantization\" : \"Q8_0\",\n",
    "    \"Temperature\": 0,\n",
    "    \"max_tokens\" : -1, #Default :4096\n",
    "    \"GPU Offload\": 34,\n",
    "    \"CPU Thread Pool Size\": 6,\n",
    "    \"Evaluation Batch Size\": 512,\n",
    "    \"Flash Attention\": False,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retorna apenas o usuários que tem o gt no candidatos.\n",
    "# implementar sobre o pipeline do projeto \n",
    "\n",
    "def get_candidate_ids_list(data, id_list, user_matrix_sim, num_u, num_i):\n",
    "    cand_ids = []\n",
    "    for i in id_list:\n",
    "        watched_movies = data[i][0].split(' | ')\n",
    "        candidate_items = utils.sort_user_filtering_items(data, watched_movies, user_matrix_sim[i], num_u, num_i)\n",
    "        if data[i][-1] in candidate_items:\n",
    "            cand_ids.append(i)\n",
    "    return cand_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_list = list(range(0, len(dataset)))\n",
    "#assert(len(id_list) == 943) # aqui é verificado se a lista possue exatamente essa quantidade\n",
    "\n",
    "# Building indexes and similarity matrices for users and movies.\n",
    "movie_idx = utils.build_moviename_index_dict(dataset)\n",
    "user_sim_matrix = utils.build_user_similarity_matrix(dataset, movie_idx)\n",
    "\n",
    "\n",
    "# para fazer a filtragem sobre os filmes \n",
    "#pop_dict = utils.build_movie_popularity_dict(dataset) \n",
    "#item_sim_matrix = utils.build_item_similarity_matrix(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0619833babe4b5d85bfa61779591afb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processando:   0%|          | 0/943 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result_pkl = recommender.recommendation_workflow(config         = config,\n",
    "                                                 dataset        = dataset,\n",
    "                                                 prompt_template= prompt_template,\n",
    "                                                 prompt_format  = prompt_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## execução varias configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de Usuários: 943\n",
      "24\n",
      "16\n",
      "16\n",
      "8\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from Utils import templates, utils, recommender\n",
    "\n",
    "config = {}\n",
    "\n",
    "## define o prompt template\n",
    "prompt_template = templates.PROMPT_TEMPLATE_2\n",
    "config.update({\"prompt_template\": prompt_template})\n",
    "\n",
    "## define o prompt para formatar a resposta final \n",
    "#prompt_format = templates.PROMPT_TEMPLATE_ESTRUCTURE\n",
    "prompt_format = \"\" # para não utilizar \n",
    "config.update({\"prompt_format\": prompt_format})\n",
    "\n",
    "# load movie lens 100k dataset\n",
    "dataset = utils.read_json(\"Data/ML100K_clean.json\")\n",
    "print(f'Quantidade de Usuários: {len(dataset)}')\n",
    "\n",
    "config.update({\n",
    "    #\"runtime\": \"ROCm llama.cpp v1.23.0\", \n",
    "    #\"runtime\": \"CPU llama.cpp v1.22.2\", # performance ruim\n",
    "    \"LLM_runtime\": \"Vulkan llama.cpp v1.23.0\", # melhor opção\n",
    "    \"dataset\": \"ml_100k\",\n",
    "    \"nsu\" : 18,     # número de usuários para filtragem colaborativa        | SSBD :12  | Default :18   | \n",
    "    \"nci\" :24,      # número de itens para filtragem colaborativa           | SSBD :19  | Default :24   | Max : 1682\n",
    "    \"lenlimit\" : 24,  # limite de tamanho para a lista filmes assistidos    | SSBD : 8  | Default :24   | Max : 1682\n",
    "    \"test_run\" : 0, # define a quantidade de recomendações,                 |           | Default :0    | Max : 943 \n",
    "    \"obs\": \"base\"\n",
    "})\n",
    "\n",
    "config.update({\n",
    "    \"model_name\" :\"gemma-3-4b-it\",\n",
    "    \"Arch\" : \"gemma3\",\n",
    "    \"Quantization\" : \"Q4_K_M\",\n",
    "    \"Temperature\": 0.1,\n",
    "    \"max_tokens\" : -1,  # Default : 4096\n",
    "    \"GPU Offload\": 34,\n",
    "    \"CPU Thread Pool Size\": 6,\n",
    "    \"Evaluation Batch Size\": 512,\n",
    "    \"Flash Attention\": False, # não vi vantagem no uso \n",
    "})\n",
    "\n",
    "config_list = []\n",
    "\n",
    "config1 = config.copy()\n",
    "config1.update({\n",
    "    \"obs\": \"testando lenlimits e temperatura\",\n",
    "    \"lenlimit\": 24,\n",
    "    \"Temperature\": 0.0})\n",
    "config_list.append(config1)\n",
    "\n",
    "config3 = config.copy()\n",
    "config3.update({\n",
    "    \"obs\": \"testando lenlimits e temperatura\",\n",
    "    \"Temperature\": 0.1,\n",
    "    \"lenlimit\": 16,\n",
    "})\n",
    "config_list.append(config3)\n",
    "\n",
    "config4 = config.copy()\n",
    "config4.update({\n",
    "    \"obs\": \"testando lenlimits e temperatura\",\n",
    "    \"Temperature\": 0.0,\n",
    "    \"lenlimit\": 16,\n",
    "})\n",
    "config_list.append(config4)\n",
    "\n",
    "config5 = config.copy()\n",
    "config5.update({\n",
    "    \"obs\": \"testando lenlimits e temperatura\",\n",
    "    \"Temperature\": 0.1,\n",
    "    \"lenlimit\": 8,\n",
    "})\n",
    "config_list.append(config5)\n",
    "\n",
    "config6 = config.copy()\n",
    "config6.update({\n",
    "    \"obs\": \"testando lenlimits e temperatura\",\n",
    "    \"Temperature\": 0.0,\n",
    "    \"lenlimit\": 8,\n",
    "})\n",
    "config_list.append(config6)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0, len(config_list)):\n",
    "    config = config_list[i]\n",
    "    print(f'Rodando configuração {i} de {len(config_list)}')\n",
    "    try:\n",
    "        result_pkl = recommender.recommendation_workflow(config         = config,\n",
    "                                                         dataset        = dataset,\n",
    "                                                        prompt_template= prompt_template,\n",
    "                                                        prompt_format  = prompt_format)\n",
    "        print(f'Configuração {i} de {len(config_list)} finalizada')\n",
    "    except Exception as e:\n",
    "        print(f'Erro na configuração {i} de {len(config_list)}')\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt_template': {'System_prompt': \"You are a movie expert provide the answer for the question based on the given context. If you don't know the answer to a question, please don't share false information.\", 'Preference': '\\n    ### MY WATCHED MOVIES LIST: {}.\\n\\n    ### QUESTION: Based on my watched movies list. Tell me what features are most important to me when selecting movies (Summarize my preferences briefly)?\\n\\n    ### ANSWER:\\n    ', 'Featured_movies': '\\n\\n    ### MY WATCHED MOVIES LIST: {}.\\n\\n    ### MY MOVIE PREFERENCES: {}.\\n\\n    ### QUESTION: Create an enumerated list selecting the five most featured movies from the watched movies according to my movie preferences.\\n\\n    ### ANSWER:\\n    ', 'Recommendation': '\\n\\n    ### CANDIDATE MOVIE SET: {}.\\n\\n    ### MY WATCHED MOVIES LIST: {}.\\n\\n    ### MY MOVIE PREFERENCES: {}.\\n\\n    ### MY FIVE MOST FEATURED MOVIES: {}.\\n\\n    ### INSTRUCTIONS:\\n    Recommend exactly **10 movies** from the \"Candidate Movie Set\" that are most similar to my \"Five Most Featured Movies\".  \\n    The output must be **STRICTLY formatted** as follows:\\n\\n    1. Movie Title  \\n    2. Movie Title  \\n    3. Movie Title  \\n    4. Movie Title  \\n    5. Movie Title  \\n    6. Movie Title  \\n    7. Movie Title  \\n    8. Movie Title  \\n    9. Movie Title  \\n    10. Movie Title  \\n\\n    Do not include any extra text before or after the list \\n    \\n    ### QUESTION:  \\n    What are your top 10 movie recommendations?  \\n\\n    ### ANSWER:\\n'}, 'prompt_format': '', 'LLM_runtime': 'Vulkan llama.cpp v1.23.0', 'dataset': 'ml_100k', 'nsu': 18, 'nci': 24, 'lenlimit': 100, 'test_run': 0, 'obs': 'testando lenlimits', 'model_name': 'gemma-3-4b-it', 'Arch': 'gemma3', 'Quantization': 'Q4_K_M', 'Temperature': 0.0, 'max_tokens': -1, 'GPU Offload': 34, 'CPU Thread Pool Size': 6, 'Evaluation Batch Size': 512, 'Flash Attention': False}\n",
      "{'prompt_template': {'System_prompt': \"You are a movie expert provide the answer for the question based on the given context. If you don't know the answer to a question, please don't share false information.\", 'Preference': '\\n    ### MY WATCHED MOVIES LIST: {}.\\n\\n    ### QUESTION: Based on my watched movies list. Tell me what features are most important to me when selecting movies (Summarize my preferences briefly)?\\n\\n    ### ANSWER:\\n    ', 'Featured_movies': '\\n\\n    ### MY WATCHED MOVIES LIST: {}.\\n\\n    ### MY MOVIE PREFERENCES: {}.\\n\\n    ### QUESTION: Create an enumerated list selecting the five most featured movies from the watched movies according to my movie preferences.\\n\\n    ### ANSWER:\\n    ', 'Recommendation': '\\n\\n    ### CANDIDATE MOVIE SET: {}.\\n\\n    ### MY WATCHED MOVIES LIST: {}.\\n\\n    ### MY MOVIE PREFERENCES: {}.\\n\\n    ### MY FIVE MOST FEATURED MOVIES: {}.\\n\\n    ### INSTRUCTIONS:\\n    Recommend exactly **10 movies** from the \"Candidate Movie Set\" that are most similar to my \"Five Most Featured Movies\".  \\n    The output must be **STRICTLY formatted** as follows:\\n\\n    1. Movie Title  \\n    2. Movie Title  \\n    3. Movie Title  \\n    4. Movie Title  \\n    5. Movie Title  \\n    6. Movie Title  \\n    7. Movie Title  \\n    8. Movie Title  \\n    9. Movie Title  \\n    10. Movie Title  \\n\\n    Do not include any extra text before or after the list \\n    \\n    ### QUESTION:  \\n    What are your top 10 movie recommendations?  \\n\\n    ### ANSWER:\\n'}, 'prompt_format': '', 'LLM_runtime': 'Vulkan llama.cpp v1.23.0', 'dataset': 'ml_100k', 'nsu': 18, 'nci': 24, 'lenlimit': 100, 'test_run': 0, 'obs': 'testando lenlimits', 'model_name': 'gemma-3-4b-it', 'Arch': 'gemma3', 'Quantization': 'Q4_K_M', 'Temperature': 0.0, 'max_tokens': -1, 'GPU Offload': 34, 'CPU Thread Pool Size': 6, 'Evaluation Batch Size': 512, 'Flash Attention': False}\n",
      "{'prompt_template': {'System_prompt': \"You are a movie expert provide the answer for the question based on the given context. If you don't know the answer to a question, please don't share false information.\", 'Preference': '\\n    ### MY WATCHED MOVIES LIST: {}.\\n\\n    ### QUESTION: Based on my watched movies list. Tell me what features are most important to me when selecting movies (Summarize my preferences briefly)?\\n\\n    ### ANSWER:\\n    ', 'Featured_movies': '\\n\\n    ### MY WATCHED MOVIES LIST: {}.\\n\\n    ### MY MOVIE PREFERENCES: {}.\\n\\n    ### QUESTION: Create an enumerated list selecting the five most featured movies from the watched movies according to my movie preferences.\\n\\n    ### ANSWER:\\n    ', 'Recommendation': '\\n\\n    ### CANDIDATE MOVIE SET: {}.\\n\\n    ### MY WATCHED MOVIES LIST: {}.\\n\\n    ### MY MOVIE PREFERENCES: {}.\\n\\n    ### MY FIVE MOST FEATURED MOVIES: {}.\\n\\n    ### INSTRUCTIONS:\\n    Recommend exactly **10 movies** from the \"Candidate Movie Set\" that are most similar to my \"Five Most Featured Movies\".  \\n    The output must be **STRICTLY formatted** as follows:\\n\\n    1. Movie Title  \\n    2. Movie Title  \\n    3. Movie Title  \\n    4. Movie Title  \\n    5. Movie Title  \\n    6. Movie Title  \\n    7. Movie Title  \\n    8. Movie Title  \\n    9. Movie Title  \\n    10. Movie Title  \\n\\n    Do not include any extra text before or after the list \\n    \\n    ### QUESTION:  \\n    What are your top 10 movie recommendations?  \\n\\n    ### ANSWER:\\n'}, 'prompt_format': '', 'LLM_runtime': 'Vulkan llama.cpp v1.23.0', 'dataset': 'ml_100k', 'nsu': 18, 'nci': 24, 'lenlimit': 100, 'test_run': 0, 'obs': 'testando lenlimits', 'model_name': 'gemma-3-4b-it', 'Arch': 'gemma3', 'Quantization': 'Q4_K_M', 'Temperature': 0.0, 'max_tokens': -1, 'GPU Offload': 34, 'CPU Thread Pool Size': 6, 'Evaluation Batch Size': 512, 'Flash Attention': False}\n",
      "{'prompt_template': {'System_prompt': \"You are a movie expert provide the answer for the question based on the given context. If you don't know the answer to a question, please don't share false information.\", 'Preference': '\\n    ### MY WATCHED MOVIES LIST: {}.\\n\\n    ### QUESTION: Based on my watched movies list. Tell me what features are most important to me when selecting movies (Summarize my preferences briefly)?\\n\\n    ### ANSWER:\\n    ', 'Featured_movies': '\\n\\n    ### MY WATCHED MOVIES LIST: {}.\\n\\n    ### MY MOVIE PREFERENCES: {}.\\n\\n    ### QUESTION: Create an enumerated list selecting the five most featured movies from the watched movies according to my movie preferences.\\n\\n    ### ANSWER:\\n    ', 'Recommendation': '\\n\\n    ### CANDIDATE MOVIE SET: {}.\\n\\n    ### MY WATCHED MOVIES LIST: {}.\\n\\n    ### MY MOVIE PREFERENCES: {}.\\n\\n    ### MY FIVE MOST FEATURED MOVIES: {}.\\n\\n    ### INSTRUCTIONS:\\n    Recommend exactly **10 movies** from the \"Candidate Movie Set\" that are most similar to my \"Five Most Featured Movies\".  \\n    The output must be **STRICTLY formatted** as follows:\\n\\n    1. Movie Title  \\n    2. Movie Title  \\n    3. Movie Title  \\n    4. Movie Title  \\n    5. Movie Title  \\n    6. Movie Title  \\n    7. Movie Title  \\n    8. Movie Title  \\n    9. Movie Title  \\n    10. Movie Title  \\n\\n    Do not include any extra text before or after the list \\n    \\n    ### QUESTION:  \\n    What are your top 10 movie recommendations?  \\n\\n    ### ANSWER:\\n'}, 'prompt_format': '', 'LLM_runtime': 'Vulkan llama.cpp v1.23.0', 'dataset': 'ml_100k', 'nsu': 18, 'nci': 24, 'lenlimit': 100, 'test_run': 0, 'obs': 'testando lenlimits', 'model_name': 'gemma-3-4b-it', 'Arch': 'gemma3', 'Quantization': 'Q4_K_M', 'Temperature': 0.0, 'max_tokens': -1, 'GPU Offload': 34, 'CPU Thread Pool Size': 6, 'Evaluation Batch Size': 512, 'Flash Attention': False}\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(config_list)):\n",
    "    config = config_list[i]\n",
    "    print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{result_pkl}', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for key, value in data.items():\n",
    "    if isinstance(key, int) and isinstance(value, dict):  # Pegando apenas os experimentos\n",
    "        results.append({\n",
    "            'Candidates': value.get('candidate_set', ''),\n",
    "            'Ground Truth': value.get('ground_truth', ''),\n",
    "            'gt_in_candidate_set': value.get('gt_in_candidate_set', ''),\n",
    "            #'Input 1': value.get('input_1', ''),\n",
    "            #'Predictions 1': value.get('predictions_1', ''),\n",
    "            #'Input 2': value.get('input_2', ''),\n",
    "            #'Predictions 2': value.get('predictions_2', ''),\n",
    "            'Input 3': value.get('input_3', ''),\n",
    "            'Predictions 3': value.get('predictions_3', ''),\n",
    "            #'Recommendations': value.get('recommendations', ''),\n",
    "            'rec_HitRate@10': value.get('rec_HitRate@10', ''),\n",
    "            #'Precision': value.get('precision', ''),\n",
    "            #'Recall': value.get('recall', ''),\n",
    "            'rec_NDCG@10': value.get('rec_NDCG@10', '')\n",
    "        })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "df_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
