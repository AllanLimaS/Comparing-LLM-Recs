{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuração inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from Utils import templates, utils, recommender, models_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configurações base\n",
    "\n",
    "config = {\n",
    "    \"LLM_runtime\": \"ROCm llama.cpp v1.31.0\", # melhor opção no momento (ultimo teste: 03/05/2025)\n",
    "    #\"LLM_runtime\": \"CPU llama.cpp v1.22.2\", # performance ruim\n",
    "    #\"LLM_runtime\": \"Vulkan llama.cpp v1.28.0\", # performance ruim\n",
    "    \"dataset\": \"ml_100k\",                                                  #| Opções 'ml_100k' e 'ml_1m'\n",
    "    \"nsu\" : 12,     # número de usuários para filtragem colaborativa        | SSBD :12  | Default :18   |            | Best = 19\n",
    "    \"nci\" :19,      # número de itens para filtragem colaborativa           | SSBD :19  | Default :24   | Max : 1682 |\n",
    "    \"lenlimit\" : 8,  # limite de tamanho para a lista filmes assistidos     | SSBD : 8  | Default :24   | Max : 1682 | Best = 8\n",
    "    \"lenlimit_option\" : 'aleatorio', # define qual a abordagem                | opções : 'ultimos', 'primeiros', 'aleatorio' | Default : 'aleatorio'\n",
    "    \"test_run\" : 0, # define a quantidade de recomendações,                 |           | Default :0    | Max : 943 \n",
    "    \"obs\": \"prompt\"\n",
    "}\n",
    "\n",
    "## define o prompt template\n",
    "prompt_template = templates.PROMPT_TEMPLATE_4\n",
    "config.update({\"prompt_template\": prompt_template})\n",
    "\n",
    "## define o prompt para formatar a resposta final \n",
    "#prompt_format = templates.PROMPT_TEMPLATE_ESTRUCTURE\n",
    "prompt_format = \"\" # para não utilizar \n",
    "config.update({\"prompt_format\": prompt_format})\n",
    "\n",
    "# load movie lens 100k dataset\n",
    "dataset = utils.read_json(f\"Data/{config['dataset']}.json\")\n",
    "print(f'Quantidade de Usuários: {len(dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.update(models_config.LlamaModels.llama_3_2_3b_instruct_Q4_K_M())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(models_config.LlamaModels.llama_3_2_3b_instruct_f16())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execução de configuração unitária"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifica configuração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executa 1 vez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pkl = recommender.recommendation_workflow_new(config         = config,\n",
    "                                                 dataset        = dataset,\n",
    "                                                 prompt_template= prompt_template,\n",
    "                                                 prompt_format  = prompt_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 vezes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "execucoes = 5\n",
    "for execucao in tqdm(range(execucoes),desc=f\"Execuções\"):\n",
    "    print(f'Execução {execucao+1} de {execucoes}')\n",
    "    result_pkl = recommender.recommendation_workflow_new(\n",
    "        config=config,\n",
    "        dataset=dataset,\n",
    "        prompt_template=prompt_template,\n",
    "        prompt_format=prompt_format\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execução multipla "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cria a lista de execuções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list = []\n",
    "\n",
    "config1 = config.copy()\n",
    "config1.update(models_config.LlamaModels.llama_3_2_3b_instruct_Q4_K_M())\n",
    "config1.update({\"dataset\" : \"ml_1m\"})\n",
    "config1.update({\"obs\" : \"ml_1m\"})\n",
    "config_list.append(config1)\n",
    "\n",
    "config2 = config.copy()\n",
    "config2.update(models_config.LlamaModels.llama_3_2_3b_instruct_Q4_K_M())\n",
    "config2.update({\"lenlimit\" : 1682})\n",
    "config2.update({\"obs\" : \"lenlimit 1682\"})\n",
    "config_list.append(config2)\n",
    "\n",
    "# esse llama 2 não está rodando\n",
    "#config3 = config.copy()\n",
    "#config3.update(models_config.LlamaModels.llama_2_7b_Q4_K_M())\n",
    "#config3.update({\"obs\" : \"base\"})\n",
    "#config_list.append(config3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### verifica lista de configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(config_list)):\n",
    "    config = config_list[i]\n",
    "    print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executa lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'prompt template: {prompt_template}')\n",
    "print(f'prompt format: {prompt_format}')\n",
    "\n",
    "### quantidade de keys no prompttemplate\n",
    "print(f'Quantidade de keys no prompt template: {len(prompt_template.keys())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "execucoes = 5\n",
    "\n",
    "for i in tqdm(range(0, len(config_list)),desc=\"Configurações\"):\n",
    "    config = config_list[i]\n",
    "    dataset = utils.read_json(f\"Data/{config['dataset']}.json\")\n",
    "    #print(f'Rodando configuração {i+1} de {len(config_list)}')\n",
    "\n",
    "    for execucao in tqdm(range(execucoes),desc=f\"Execuções da configuração {i+1}\"):\n",
    "        print(f'Execução {execucao+1} de {execucoes}')\n",
    "        try:\n",
    "            result_pkl = recommender.recommendation_workflow_new(config         = config,\n",
    "                                                            dataset        = dataset,\n",
    "                                                            prompt_template= prompt_template,\n",
    "                                                            prompt_format  = prompt_format)\n",
    "            print(f'Execução {execucao+1} de {execucoes} finalizada')\n",
    "        except Exception as e:\n",
    "            print(f'Erro na Execução {execucao+1} da configuração {i+1}')\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "    print(f'Configuração {i+1} de {len(config_list)} finalizada')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# outros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realação de usuários e usuários com GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users,gt_users = utils.total_users_and_gt_users(config)\n",
    "print(f'Quantidade de usuários: {users}')\n",
    "print(f'Quantidade de usuários com gt: {gt_users}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparar dataset / dividir em treino e teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cria uma lista dos usuários com GT na lista de candidatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "from Utils import templates, utils\n",
    "\n",
    "nsu = 12\n",
    "nci = 19\n",
    "lenlimit = 8 \n",
    "\n",
    "dataset = utils.read_json(\"Data/ML100K_clean.json\")\n",
    "movie_idx = utils.build_moviename_index_dict(dataset)\n",
    "user_sim_matrix = utils.build_user_similarity_matrix(dataset, movie_idx)\n",
    "id_list = list(range(0, len(dataset)))\n",
    "\n",
    "data_list = []\n",
    "id_list_com_gt_no_candidate=[]\n",
    "\n",
    "for i in id_list:\n",
    "\n",
    "  watched_mv = dataset[i][0].split(' | ')[::-1]\n",
    "  watched_mv = watched_mv[-lenlimit:]\n",
    "  \n",
    "  groundTruth = dataset[i][-1]\n",
    "  candidate_items = utils.sort_collaborative_user_filtering(target_user_id=i,\n",
    "                                                                    dataset=dataset,\n",
    "                                                                    user_similarity_matrix=user_sim_matrix,\n",
    "                                                                    num_users=nsu,\n",
    "                                                                    num_items=nci,\n",
    "                                                                    include_similar_user_GT=False,\n",
    "                                                                    debug=False)\n",
    "  random.shuffle(candidate_items)\n",
    "\n",
    "  # verifica se o ground_truth está no candidate_set\n",
    "  gt_in_candidate_set = True if any(groundTruth.lower() in candidate.lower() for candidate in candidate_items) else False\n",
    "\n",
    "  if gt_in_candidate_set == True:\n",
    "    id_list_com_gt_no_candidate.append(i)\n",
    "\n",
    "  data_list.append({\n",
    "        'user_id': i,\n",
    "        'watched_movies': watched_mv,\n",
    "        'ground_truth': groundTruth,\n",
    "        'candidate_items': candidate_items,\n",
    "        'gt_in_candidate_set': gt_in_candidate_set\n",
    "  })\n",
    "\n",
    "df = pd.DataFrame(data_list)\n",
    "\n",
    "print(f'nsu: {nsu} \\nnci: {nci} \\nQnt de usuários do dataset: {len(dataset)} \\nQnt de usuários com gt no candidate: {len(id_list_com_gt_no_candidate)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realiza a divisão de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a proporção de divisão\n",
    "train_ratio = 0.8\n",
    "test_ratio = 0.2\n",
    "\n",
    "# Embaralha os IDs para garantir aleatoriedade\n",
    "random.shuffle(id_list_com_gt_no_candidate)\n",
    "\n",
    "# Calcula o tamanho de cada conjunto\n",
    "total_ids = len(id_list_com_gt_no_candidate)\n",
    "train_size = int(train_ratio * total_ids)\n",
    "test_size = total_ids - train_size\n",
    "\n",
    "# Separa os IDs em conjuntos\n",
    "train_ids = id_list_com_gt_no_candidate[:train_size]\n",
    "test_ids = id_list_com_gt_no_candidate[train_size:]\n",
    "\n",
    "# Imprime o tamanho de cada conjunto para verificação\n",
    "print(f\"Tamanho do conjunto de treino: {len(train_ids)}\")\n",
    "print(f\"Tamanho do conjunto de teste: {len(test_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajusta o dataset de treino, adicionando as instruções necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = []\n",
    "\n",
    "# configs \n",
    "model_name = \"gemma-3-4b-it\"\n",
    "temperature = 0\n",
    "prompt_template = templates.PROMPT_TEMPLATE_3\n",
    "system_prompt = \"You are a movie expert provide the answer for the question based on the given context. Your answer must be short \" \n",
    "max_tokens = -1\n",
    "lenlimit = 8\n",
    "\n",
    "for id in train_ids:\n",
    "    filtered = df[df['user_id'] == id]\n",
    "    if not filtered.empty:\n",
    "        candidate_items = filtered['candidate_items'].iloc[0]\n",
    "        ground_truth = filtered['ground_truth'].iloc[0]\n",
    "        watched_movies = filtered['watched_movies'].iloc[0]\n",
    "    else:\n",
    "        continue \n",
    "\n",
    "    input_prompt = prompt_template['Preference'].format(', '.join(candidate_items),', '.join(watched_movies))\n",
    "    response = utils.query_lm_studio(model_name,0.0,system_prompt,input_prompt,max_tokens)\n",
    "    movie_preference = response\n",
    "\n",
    "    input_prompt = prompt_template['Featured_movies'].format(', '.join(watched_mv), movie_preference)\n",
    "    response = utils.query_lm_studio(model_name,0.0,system_prompt,input_prompt,max_tokens)\n",
    "    most_featured = response\n",
    "\n",
    "    input = templates.TRAIN_TEMPLATE_2['INPUT'].format(', '.join(candidate_items),', '.join(watched_movies),movie_preference,most_featured )\n",
    "    output= templates.TRAIN_TEMPLATE_2['OUTPUT'].format(ground_truth)\n",
    "\n",
    "    train_dataset.append({\n",
    "        'input': input,\n",
    "        'output': output\n",
    "  })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(train_dataset)\n",
    "df_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salva arquivo de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_pickle('Data/ML100K_train.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salva arquivo de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = []\n",
    "\n",
    "for id in test_ids:\n",
    "    filtered = df[df['user_id'] == id]\n",
    "    if not filtered.empty:\n",
    "        candidate_items = filtered['candidate_items'].iloc[0]\n",
    "        ground_truth = filtered['ground_truth'].iloc[0]\n",
    "        watched_movies = filtered['watched_movies'].iloc[0]\n",
    "    else:\n",
    "        continue \n",
    "    \n",
    "    test_dataset.append({\n",
    "        'user_id': id,\n",
    "        'candidate_items': candidate_items,\n",
    "        'ground_truth': ground_truth,\n",
    "        'watched_movies': watched_movies\n",
    "    })\n",
    "df_test = pd.DataFrame(test_dataset)\n",
    "df_test.to_pickle('Data/ML100K_test.pkl')\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recomendação com o dataset de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from Utils import templates, utils, recommender, models_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configurações base\n",
    "\n",
    "config = {\n",
    "    \"LLM_runtime\": \"ROCm llama.cpp v1.31.0\", # melhor opção no momento (ultimo teste: 03/05/2025)\n",
    "    #\"LLM_runtime\": \"CPU llama.cpp v1.22.2\", # performance ruim\n",
    "    #\"LLM_runtime\": \"Vulkan llama.cpp v1.28.0\", # performance ruim\n",
    "    \"dataset\": \"ML100K_test\",                                                  #| Opções 'ml_100k' e 'ml_1m'\n",
    "    \"nsu\" : 12,     # número de usuários para filtragem colaborativa        | SSBD :12  | Default :18   |            | Best = 19\n",
    "    \"nci\" :19,      # número de itens para filtragem colaborativa           | SSBD :19  | Default :24   | Max : 1682 |\n",
    "    \"lenlimit\" : 8,  # limite de tamanho para a lista filmes assistidos     | SSBD : 8  | Default :24   | Max : 1682 | Best = 8\n",
    "    \"lenlimit_option\" : 'aleatorio', # define qual a abordagem                | opções : 'ultimos', 'primeiros', 'aleatorio' | Default : 'aleatorio'\n",
    "    \"test_run\" : 0, # define a quantidade de recomendações,                 |           | Default :0    | Max : 943 \n",
    "    \"obs\": \"FT\"\n",
    "}\n",
    "\n",
    "## define o prompt template\n",
    "prompt_template = templates.PROMPT_TEMPLATE_3\n",
    "config.update({\"prompt_template\": prompt_template})\n",
    "\n",
    "## define o prompt para formatar a resposta final \n",
    "#prompt_format = templates.PROMPT_TEMPLATE_ESTRUCTURE\n",
    "prompt_format = \"\" # para não utilizar \n",
    "config.update({\"prompt_format\": prompt_format})\n",
    "\n",
    "# load movie lens 100k dataset\n",
    "dataset_path = (f\"Data/{config['dataset']}.pkl\")\n",
    "with open(dataset_path, 'rb') as f:\n",
    "    dataset = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.update(models_config.LlamaModels.llama_3_2_3b_instruct_Q4_K_M_FN())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.update(models_config.LlamaModels.llama_3_2_3b_instruct_Q4_K_M())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "execucoes = 5\n",
    "for execucao in tqdm(range(execucoes),desc=f\"Execuções\"):\n",
    "    print(f'Execução {execucao+1} de {execucoes}')\n",
    "    result_pkl = recommender.recommendation_workflow_pkl(\n",
    "        config=config,\n",
    "        dataset=dataset,\n",
    "        prompt_template=prompt_template,\n",
    "        prompt_format=prompt_format\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recomendação com filtragem colaborativa apenas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from Utils import templates, utils, recommender, models_config\n",
    "\n",
    "## Configurações base\n",
    "\n",
    "config = {\n",
    "    \"model_name\": \"Tradicional\",\n",
    "    \"dataset\": \"ml_100k\",                                                  #| Opções 'ml_100k' e 'ml_1m'\n",
    "    \"nsu\" : 12,     # número de usuários para filtragem colaborativa        | SSBD :12  | Default :18   |            | Best = 19\n",
    "    \"nci\" :19,      # número de itens para filtragem colaborativa           | SSBD :19  | Default :24   | Max : 1682 |\n",
    "    \"test_run\" : 0, # define a quantidade de recomendações,                 |           | Default :0    | Max : 943 \n",
    "}\n",
    "\n",
    "# load movie lens 100k dataset\n",
    "dataset = utils.read_json(f\"Data/{config['dataset']}.json\")\n",
    "print(f'Quantidade de Usuários: {len(dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = recommender.recommendation_workflow_tradicional(config,dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(candidates)\n",
    "top10_str = \"\\n\".join([f\"{i+1}. {filme}\" for i, filme in enumerate(candidates[:10])])\n",
    "\n",
    "print(top10_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_dcg(gain, pos):\n",
    "    \"\"\"\n",
    "    Essa função calcula o Discounted Cumulative Gain (DCG), que mede a qualidade da ordenação das recomendações, penalizando itens relevantes que aparecem em posições mais baixas da lista.\n",
    "    \n",
    "    Como funciona:\n",
    "\n",
    "    1 - Recebe o ganho de relevância (gain, geralmente 1 para itens relevantes).\n",
    "\n",
    "    2 - Divide pelo logaritmo da posição do item na lista (log2(1+pos)).\n",
    "\n",
    "    3 - Quanto menor a posição (pos mais próximo do topo da lista), maior o impacto na pontuação.\n",
    "    \"\"\"\n",
    "    return gain / math.log2(1+pos)  # DCG is accumulated of 'relevance / log2(pos + 1)'\n",
    "\n",
    "def calculate_ndcg_new(query, relevants):\n",
    "    \"\"\"\n",
    "    query = lista de itens recomendados (ordenados por relevância).\n",
    "    relevants = lista de itens relevantes (ground-truths).\n",
    "    \"\"\"\n",
    "    dcg = 0\n",
    "    idcg = 0\n",
    "\n",
    "    relevant = relevants[0]\n",
    "    for i,item in enumerate(query):\n",
    "        print(f\"Item: {item}, Posição: {i+1}, Relevante: {relevant}, len(relevants): {len(relevants)}\")\n",
    "        idcg += calculate_dcg(1,i+1) if i < len(relevants) else 0 # Calcula o IDCG se houver relevantes, senão, apenas não muda o IDCG já calculado -- a relevância dos itens corretos (ground-truths) é 1 quando existir.\n",
    "        #idcg += calculate_dcg(1,i+1) if i < len(relevant) else 0 # Calcula o IDCG se houver relevantes, senão, apenas não muda o IDCG já calculado -- a relevância dos itens corretos (ground-truths) é 1 quando existir.\n",
    "        dcg += calculate_dcg(1,i+1) if relevant in item else 0 # Calcula o DCG se o item recomendado está na lista de relevantes (ground-truths), senão, não muda o DCG -- a relevância dos itens corretos (GT) é 1\n",
    "        print(f\"DCG: {dcg}, IDCG: {idcg}\\n\")\n",
    "    return dcg / idcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils import metrics\n",
    "\n",
    "query= [\"Sabrina\",\"Jerry Maguire\", \"Birdcage\", \"Pulp Fiction\", \"Cool Hand Luke\", \"Taxi Driver\", \"Dead Poets Society\", \"M*A*S*H\", \"GT\", \"GoodFellas\"]\n",
    "relevant = [\"GT\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils import metrics\n",
    "\n",
    "query= [\"Sabrina\",\"Jerry Maguire\", \"Birdcage\", \"Pulp Fiction\", \"GT\"]\n",
    "relevant = [\"GT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils import metrics\n",
    "\n",
    "query= [\"a\",\"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"w\", \"h\", \"l\"]\n",
    "relevant = [\"h\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ndcg_at_k(ranked_list, ground_truth):\n",
    "    \"\"\"\n",
    "    ranked_list: lista de itens recomendados ordenados (ex: ['A', 'B', 'C', ...])\n",
    "    ground_truth: item relevante real (ex: 'C')\n",
    "    k: cutoff (ex: 5 ou 10)\n",
    "    \"\"\"\n",
    "    dcg = 0.0\n",
    "    for i in range(min(10, len(ranked_list))):\n",
    "        if ranked_list[i] == ground_truth:\n",
    "            dcg = 1 / np.log2(i + 2)  # +2 porque o log2 começa do 2 na fórmula\n",
    "            break  # só um item relevante, pode parar aqui\n",
    "\n",
    "    idcg = 1 / np.log2(1 + 1)  # ideal: item relevante na posição 1\n",
    "    ndcg = dcg / idcg\n",
    "    return ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item: Sabrina, Posição: 1, Relevante: GT, len(relevants): 1\n",
      "DCG: 0, IDCG: 1.0\n",
      "\n",
      "Item: Jerry Maguire, Posição: 2, Relevante: GT, len(relevants): 1\n",
      "DCG: 0, IDCG: 1.0\n",
      "\n",
      "Item: Birdcage, Posição: 3, Relevante: GT, len(relevants): 1\n",
      "DCG: 0, IDCG: 1.0\n",
      "\n",
      "Item: Pulp Fiction, Posição: 4, Relevante: GT, len(relevants): 1\n",
      "DCG: 0, IDCG: 1.0\n",
      "\n",
      "Item: GT, Posição: 5, Relevante: GT, len(relevants): 1\n",
      "DCG: 0.38685280723454163, IDCG: 1.0\n",
      "\n",
      "NDCG: 0.38685280723454163\n"
     ]
    }
   ],
   "source": [
    "ndcg =calculate_ndcg_new(query, relevant)\n",
    "print(f\"NDCG: {ndcg}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
